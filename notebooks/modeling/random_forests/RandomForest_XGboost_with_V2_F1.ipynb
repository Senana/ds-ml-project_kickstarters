{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## RandomForest with different features\n",
    "Trying out the alternative feature engineering from Linear regression and going for a second version.\n",
    "\n",
    "Experiment: What changes if we use time-series based evaluation (test-split) rather than regular? \n",
    "\n",
    "#### Second Exploration\n",
    "Try try using the exact same RandomForest modelling and optimizing as before and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial imports \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get the absolute path of the current file/notebook\n",
    "# If using Jupyter, use Path.cwd(). If using a .py script, use Path(__file__).parent\n",
    "curr_dir = Path.cwd()\n",
    "\n",
    "# Calculate the project root (adjust '.parent' count as needed)\n",
    "# If your notebook is in 'project/notebooks/', the root is 1 level up\n",
    "project_root = curr_dir.parent.parent \n",
    "\n",
    "# Add project root to system path so Python can find 'utils'\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project Root added to path: {project_root}\")\n",
    "\n",
    "from utils.feature_engineer_df import build_features \n",
    "\n",
    "#for the scaling and encoding \n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    precision_recall_curve\n",
    ")\n",
    "#cleanup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "V2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the project root\n",
    "curr_dir = Path.cwd()\n",
    "project_root = curr_dir.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Load cleaned data\n",
    "cleaned_path = project_root / \"data\" / \"cleaned\" / \"kickstarter_cleaned.csv\"\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(cleaned_path, encoding='latin-1', low_memory=False)\n",
    "print(f\"Loaded cleaned dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "\n",
    "# Convert launched to datetime\n",
    "df['launched'] = pd.to_datetime(df['launched'], errors='coerce')\n",
    "df['deadline'] = pd.to_datetime(df['deadline'], errors='coerce')\n",
    "\n",
    "# Filter only on launch-time valid fields (NOT pledged/backers)\n",
    "df = df[df['usd_goal_real'] > 0].copy()\n",
    "df = df[df['launched'].notna()].copy()\n",
    "df = df[df['deadline'].notna()].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering (goal > 0, valid dates): {df.shape[0]:,} rows\")\n",
    "print(\"Target distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Success rate: {df['target'].mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Feature Engineering (Recreate from 02_feature_engineering.ipynb)\n",
    "\n",
    "We'll recreate the same preprocessing steps to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create basic categorical features BEFORE time split\n",
    "# These don't depend on aggregates, so it is safe to create before splitting\n",
    "\n",
    "# Create main_category_grouped\n",
    "category_map = {\n",
    "    'Art': 'Creative', 'Comics': 'Creative', 'Crafts': 'Creative',\n",
    "    'Dance': 'Creative', 'Design': 'Creative',\n",
    "    'Fashion': 'Consumer', 'Food': 'Consumer',\n",
    "    'Film & Video': 'Entertainment', 'Games': 'Entertainment',\n",
    "    'Music': 'Entertainment', 'Theater': 'Entertainment',\n",
    "    'Photography': 'Creative', 'Publishing': 'Creative',\n",
    "    'Technology': 'Tech', 'Journalism': 'Other'\n",
    "}\n",
    "df['main_category_grouped'] = df['main_category'].map(category_map).fillna('Other')\n",
    "\n",
    "# Create continent\n",
    "continent_map = {\n",
    "    'US': 'North America', 'CA': 'North America', 'MX': 'North America',\n",
    "    'GB': 'Europe', 'DE': 'Europe', 'FR': 'Europe', 'IT': 'Europe',\n",
    "    'ES': 'Europe', 'NL': 'Europe', 'IE': 'Europe', 'SE': 'Europe',\n",
    "    'CH': 'Europe', 'AT': 'Europe', 'DK': 'Europe', 'BE': 'Europe',\n",
    "    'LU': 'Europe', 'NO': 'Europe',\n",
    "    'AU': 'Oceania', 'NZ': 'Oceania',\n",
    "    'JP': 'Asia', 'SG': 'Asia', 'HK': 'Asia'\n",
    "}\n",
    "df['continent'] = df['country'].map(continent_map).fillna('Other')\n",
    "\n",
    "# Create month features (from launch/deadline dates)\n",
    "df['launched_month'] = df['launched'].dt.month\n",
    "df['deadline_month'] = df['deadline'].dt.month\n",
    "\n",
    "# Add weekday feature\n",
    "df['launched_weekday'] = df['launched'].dt.weekday\n",
    "\n",
    "print(\"Created basic categorical features:\")\n",
    "print(f\"  - main_category_grouped: {df['main_category_grouped'].nunique()} categories\")\n",
    "print(f\"  - continent: {df['continent'].nunique()} categories\")\n",
    "print(f\"  - launched_month: {df['launched_month'].nunique()} months (1-12)\")\n",
    "print(f\"  - launched_weekday: {df['launched_weekday'].nunique()} weekdays (0=Monday, 6=Sunday)\")\n",
    "\n",
    "# Step 2: Time based split\n",
    "# Sort by launch date\n",
    "df_sorted = df.sort_values('launched').copy()\n",
    "\n",
    "# Use 70% earliest projects for training, 30% latest for testing\n",
    "split_idx = int(len(df_sorted) * 0.7)\n",
    "df_train_raw = df_sorted.iloc[:split_idx].copy()\n",
    "df_test_raw = df_sorted.iloc[split_idx:].copy()\n",
    "\n",
    "print(\"\\nTime-based split:\")\n",
    "print(f\"Train: {df_train_raw['launched'].min()} to {df_train_raw['launched'].max()}\")\n",
    "print(f\"Test:  {df_test_raw['launched'].min()} to {df_test_raw['launched'].max()}\")\n",
    "print(f\"\\nTrain size: {len(df_train_raw):,} ({len(df_train_raw)/len(df_sorted):.1%})\")\n",
    "print(f\"Test size:  {len(df_test_raw):,} ({len(df_test_raw)/len(df_sorted):.1%})\")\n",
    "print(f\"\\nTrain success rate: {df_train_raw['target'].mean():.2%}\")\n",
    "print(f\"Test success rate:  {df_test_raw['target'].mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare features\n",
    "# usd_goal_real spans 8 orders of magnitude - log transform is really important\n",
    "df_train_raw['log_usd_goal'] = np.log1p(df_train_raw['usd_goal_real'])\n",
    "df_test_raw['log_usd_goal'] = np.log1p(df_test_raw['usd_goal_real'])\n",
    "\n",
    "# Add goal_per_day feature (intensity: goal per day)\n",
    "# \"I need CHF 50k in 10 days\" is different from \"in 60 days\"\n",
    "df_train_raw['goal_per_day'] = df_train_raw['log_usd_goal'] / (df_train_raw['duration_days'] + 1)\n",
    "df_test_raw['goal_per_day'] = df_test_raw['log_usd_goal'] / (df_test_raw['duration_days'] + 1)\n",
    "\n",
    "# Columns to drop (keep 'launched' temporarily for train/val split)\n",
    "columns_to_drop = [\n",
    "    'id', 'main_category', 'deadline',\n",
    "    'backers', 'usd_pledged_real', 'usd_pledged_bins', 'backers_per_pledged', \n",
    "    'backer_pledged_bins', 'pledged_per_category',\n",
    "    'launched_year', 'deadline_year', 'usd_goal_real',\n",
    "    'usd_goal_bins', 'category_goal_percentile', 'duration_bins',\n",
    "    'country',\n",
    "    'deadline_month',\n",
    "]\n",
    "\n",
    "# Prepare feature sets\n",
    "df_train_clean = df_train_raw.drop(columns=[col for col in columns_to_drop if col in df_train_raw.columns])\n",
    "df_test_clean = df_test_raw.drop(columns=[col for col in columns_to_drop if col in df_test_raw.columns])\n",
    "\n",
    "print(\"Features after cleaning:\")\n",
    "print(f\"Train: {df_train_clean.shape[1]} columns\")\n",
    "print(f\"Test:  {df_test_clean.shape[1]} columns\")\n",
    "print(f\"\\nFeature columns: {[c for c in df_train_clean.columns if c != 'target']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split train into train/val for threshold optimization\n",
    "# This prevents overfitting threshold choice to training data\n",
    "df_train_clean_sorted = df_train_clean.sort_values('launched').copy()\n",
    "val_split_idx = int(len(df_train_clean_sorted) * 0.8)  # 80% train, 20% val\n",
    "\n",
    "df_train_final = df_train_clean_sorted.iloc[:val_split_idx].copy()\n",
    "df_val_final = df_train_clean_sorted.iloc[val_split_idx:].copy()\n",
    "\n",
    "print(\"Train/Val split (time-based):\")\n",
    "print(f\"Train: {df_train_final['launched'].min()} to {df_train_final['launched'].max()}\")\n",
    "print(f\"Val:    {df_val_final['launched'].min()} to {df_val_final['launched'].max()}\")\n",
    "print(f\"\\nTrain size: {len(df_train_final):,} ({len(df_train_final)/len(df_train_clean_sorted):.1%})\")\n",
    "print(f\"Val size:   {len(df_val_final):,} ({len(df_val_final)/len(df_train_clean_sorted):.1%})\")\n",
    "print(f\"Test size:  {len(df_test_clean):,}\")\n",
    "\n",
    "# Now drop 'launched' and separate X and y\n",
    "X_train = df_train_final.drop(columns=['target', 'launched'])\n",
    "y_train = df_train_final['target']\n",
    "X_val = df_val_final.drop(columns=['target', 'launched'])\n",
    "y_val = df_val_final['target']\n",
    "X_test = df_test_clean.drop(columns=['target', 'launched'])\n",
    "y_test = df_test_clean['target']\n",
    "\n",
    "print(\"\\nFinal feature matrices:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(\"\\nTarget distributions:\")\n",
    "print(f\"Train: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Val:   {y_val.value_counts().to_dict()}\")\n",
    "print(f\"Test:  {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create preprocessing pipeline using ColumnTransformer\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = ['log_usd_goal', 'duration_days', 'goal_per_day', 'launched_weekday']\n",
    "categorical_features = ['launched_month', 'main_category_grouped', 'continent']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any other columns\n",
    ")\n",
    "\n",
    "# Fit on training data and transform all sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names\n",
    "numeric_names = numeric_features\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "cat_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "feature_names = list(numeric_names) + list(cat_names)\n",
    "\n",
    "# Convert to DataFrames for easier inspection\n",
    "X_train_final = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "X_val_final = pd.DataFrame(X_val_processed, columns=feature_names, index=X_val.index)\n",
    "X_test_final = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "print(f\"X_train_final: {X_train_final.shape}\")\n",
    "print(f\"X_val_final:   {X_val_final.shape}\")\n",
    "print(f\"X_test_final:  {X_test_final.shape}\")\n",
    "print(f\"\\nFeature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Prepare for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Different steps than before - all done above "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Get a baseline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = df_train_final.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "#works out of the box \n",
    "plt.tick_params(axis='x', labelrotation=45)\n",
    "#add tick rotation if needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "using the train-test-split: \n",
    "* X_train_final\n",
    "* X_val_final\n",
    "* X_test_final\n",
    "* y_train\n",
    "* y_val  \n",
    "* y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features for feature importance \n",
    "features = list(X_train_final.columns)\n",
    "#create a model\n",
    "model = RandomForestClassifier(n_estimators = 50, #start low\n",
    "                               random_state = 42, #just any\n",
    "                               max_features=8, #manually set it to 1/2 dataset \n",
    "                               n_jobs=-1, #use whole CPU\n",
    "                               )\n",
    "#fit the model\n",
    "model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first description of the model \n",
    "n_nodes = []\n",
    "max_depths = []\n",
    "#create stats \n",
    "for tree in model.estimators_:\n",
    "  n_nodes.append(tree.tree_.node_count) #ask the tree how many nodes it has\n",
    "  max_depths.append(tree.tree_.max_depth)\n",
    "#get averages  \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the original\n",
    "train_predictions = model.predict(X_train_final)\n",
    "train_probabilites = model.predict_proba(X_train_final)[:, 1] #get the positives\n",
    "#check with test: \n",
    "test_predictions = model.predict(X_test_final)\n",
    "test_probabilities = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probabilites)}')\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, test_probabilities)}')\n",
    "print(f\"Baseline ROC AUC calling everyone successful: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Original scores V1: \n",
    "* Train ROC AUC Score: 0.9344639720579019\n",
    "* Test ROC AUC Score: 0.6197402656005501\n",
    "* Baseline ROC AUC calling everyone successful: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_model = pd.DataFrame({'feature': features,\n",
    "                         'importance': model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "fi_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### one shot at optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter grid \n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 40, 50, 70, 90, 120], #just because I can, fibonacci-style guess\n",
    "    'max_depth': [None] + list(np.arange(3, 21).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None, 8], \n",
    "    'max_leaf_nodes': [20, 100, 1000, 3000, 8000, 20000, 60000, 100000],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "#get an instance to use for random search:\n",
    "estimator = RandomForestClassifier(random_state = 42)\n",
    "#create random search model \n",
    "rs = RandomizedSearchCV(estimator,\n",
    "                        param_grid, \n",
    "                        n_jobs = -1,\n",
    "                        scoring = 'roc_auc',\n",
    "                        cv = 5,\n",
    "                        n_iter = 10,\n",
    "                        verbose = 4, \n",
    "                        random_state = 42)\n",
    "\n",
    "# fit it \n",
    "rs.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Original best parameters V1\n",
    "* {'n_estimators': 20,\n",
    "*  'min_samples_split': 10,\n",
    "* 'max_leaf_nodes': 100,\n",
    "* 'max_features': 8,\n",
    "*  'max_depth': 19,\n",
    "*  'bootstrap': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### go for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_predictions = best_model.predict(X_train_final)\n",
    "best_train_probas = best_model.predict_proba(X_train_final)[:, 1]\n",
    "#and again \n",
    "best_test_predictions = best_model.predict(X_test_final)\n",
    "best_test_probas = best_model.predict_proba(X_test_final)[:, 1]\n",
    "# and validate\n",
    "best_val_predictions = best_model.predict(X_val_final)\n",
    "best_val_probas = best_model.predict_proba(X_val_final)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now after optimization\n",
    "b_n_nodes = []\n",
    "b_max_depths = []\n",
    "\n",
    "for tree in best_model.estimators_:\n",
    "    b_n_nodes.append(tree.tree_.node_count)\n",
    "    b_max_depths.append(tree.tree_.max_depth)\n",
    "\n",
    "print(f'Average number of nodes {int(np.mean(b_n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(b_max_depths))}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cm = confusion_matrix(y_test, best_test_predictions)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Original CF: \n",
    "* [[31815 15904]\n",
    "* [20256 19931]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, best_train_probas)}')\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, best_test_probas)}')\n",
    "print(f'Val ROC AUC Score: {roc_auc_score(y_val, best_val_probas)}')\n",
    "print(f\"Baseline ROC AUC calling everyone successful: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Original scores using V1: \n",
    "* Train ROC AUC Score: 0.7050986554975087\n",
    "* Test ROC AUC Score: 0.6967549518716112\n",
    "* Baseline ROC AUC calling everyone successful: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_model = pd.DataFrame({'feature': features,\n",
    "                         'importance': best_model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "fi_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Final try with other Ensemble: \n",
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "using the train-test-split: \n",
    "* X_train_final\n",
    "* X_val_final\n",
    "* X_test_final\n",
    "* y_train\n",
    "* y_val  \n",
    "* y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_clf = RandomForestClassifier(random_state=42)\n",
    "# rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# print(\"Random Forest on Not-Reduced Features:\")\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "# print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
    "# print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
    "# print(\"F1 Score:\", f1_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_clf.predict(X_test_reduced)\n",
    "\n",
    "print(\"XGBoost on Reduced Features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_xgb))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_xgb))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "#xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "#y_pred_xgb = xgb_clf.predict(X_test_reduced)\n",
    "\n",
    "#print(\"XGBoost on Not_Reduced Features:\")\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "#print(\"Precision:\", precision_score(y_test, y_pred_xgb))\n",
    "#print(\"Recall:\", recall_score(y_test, y_pred_xgb))\n",
    "#print(\"F1 Score:\", f1_score(y_test, y_pred_xgb))\n",
    "\n",
    "#ValueError: feature_names must be string, and may not contain [, ] or\n",
    "#the line xgb_clf.fit(X_train, y_train)  is the issue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
