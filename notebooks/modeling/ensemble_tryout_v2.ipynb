{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436a0140",
   "metadata": {},
   "source": [
    "## template for first exploration \n",
    "Scaling version 1 from 02_feature_engineering\n",
    "used dummies for pure categoricals, kept numericals and scaled them using StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fcf07",
   "metadata": {},
   "source": [
    "### CHANGE ME \n",
    "Let's plug in the final feature engineering in the following section \n",
    "(left in for having something to work with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008c8655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root added to path: /Users/surya/Documents/neue_fische/11_project/ds-ml-project_kickstarters\n"
     ]
    }
   ],
   "source": [
    "#initial imports \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get the absolute path of the current file/notebook\n",
    "# If using Jupyter, use Path.cwd(). If using a .py script, use Path(__file__).parent\n",
    "curr_dir = Path.cwd()\n",
    "\n",
    "# Calculate the project root (adjust '.parent' count as needed)\n",
    "# If your notebook is in 'project/notebooks/', the root is 1 level up\n",
    "project_root = curr_dir.parent.parent \n",
    "\n",
    "# Add project root to system path so Python can find 'utils'\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project Root added to path: {project_root}\")\n",
    "\n",
    "from utils.feature_engineer_df import build_features \n",
    "\n",
    "#for the scaling and encoding \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#cleanup \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53d381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting feature engineering pipeline\n",
      "INFO:__main__:Loaded 331675 rows\n",
      "INFO:__main__:Final columns before save: ['id', 'main_category', 'deadline', 'launched', 'backers', 'country', 'usd_pledged_real', 'usd_goal_real', 'duration_days', 'target', 'main_category_grouped', 'continent', 'launched_year', 'launched_month', 'deadline_year', 'deadline_month', 'usd_goal_bins', 'usd_pledged_bins', 'pledged_per_category', 'goal_per_category', 'category_goal_percentile', 'duration_bins', 'backers_per_pledged', 'backer_pledged_bins', 'launch_season', 'deadline_season']\n",
      "INFO:__main__:Saved engineered dataset to /Users/surya/Documents/neue_fische/11_project/ds-ml-project_kickstarters/data/feature/kickstarter_featured.csv\n"
     ]
    }
   ],
   "source": [
    "#get your data from our utils\n",
    "build_features(\n",
    "    input_path=Path.cwd().resolve().parents[1] / \"data\" / \"cleaned\" / \"kickstarter_cleaned.csv\",\n",
    "    output_path=Path.cwd().resolve().parents[1] / \"data\" / \"feature\" / \"kickstarter_featured.csv\",\n",
    "    raw_path=Path.cwd().resolve().parents[1] / \"data\" / \"raw\" / \"ks-projects-201801.csv\",\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Load Files as DataFrames\n",
    "BASE_DIR = Path.cwd().resolve().parents[1]\n",
    "data_file = BASE_DIR / \"data\" / \"feature\" / \"kickstarter_featured.csv\"\n",
    "\n",
    "filepath = Path(data_file)\n",
    "\n",
    "df = pd.read_csv(filepath, encoding='latin-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adf6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns to \"hard drop\" from feature engineering dataframe\n",
    "columns_to_drop = ['id', #irrelevant\n",
    "                   'main_category', #substituted in a satisfactory way\n",
    "                   'deadline', 'launched', #created new categories \n",
    "                   'backers', 'usd_pledged_real', 'usd_pledged_bins', 'backers_per_pledged', 'backer_pledged_bins', 'pledged_per_category', #everything to do with \"future information\"\n",
    "                   'launched_year', 'deadline_year', #info about the past and not seasonal\n",
    "                   ]\n",
    "# drop them\n",
    "dfc = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3691841",
   "metadata": {},
   "source": [
    "Get dummies for pure categoricals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aef2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#season launched, dropping first as it's multicollinear\n",
    "df_sl = pd.get_dummies(df['launch_season'], prefix = 'sl_', drop_first=True, dtype=int)\n",
    "\n",
    "#season deadline, dropping first\n",
    "df_sd = pd.get_dummies(df['deadline_season'], prefix = 'sd_', drop_first=True, dtype=int)\n",
    "\n",
    "#main category_grouped, dropping first\n",
    "df_cat = pd.get_dummies(df['main_category_grouped'], prefix = 'cat_', drop_first=True, dtype=int)\n",
    "\n",
    "#continent, dropping first\n",
    "df_co = pd.get_dummies(df['continent'], prefix = 'co_', drop_first=True, dtype=int)\n",
    "\n",
    "#put everything back together again: \n",
    "dff = pd.concat([dfc, df_sl, df_sd, df_cat, df_co], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "280b6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's soft-drop everything we just encoded\n",
    "#commented out all the keepers \n",
    "columns_to_softdrop = ['country', #we kept it for comparison\n",
    "                       # 'usd_goal_real', #right now I want to try scaling actual values \n",
    "                       #'duration_days', #I want to scale these and drop the bins instead for now \n",
    "                       # 'target', (obviously)\n",
    "                       'main_category_grouped', 'continent', #after creating dummies, get rid of these!\n",
    "                        'launched_month', 'deadline_month', #because we have season but might want to look closer\n",
    "                        'usd_goal_bins', #using category_goal_percentile (those two are redundant)\n",
    "                        #'goal_per_category', #it's a polynomial feature - not independent but that's probably ok\n",
    "                       'category_goal_percentile', #it's an orinal bin so keeping 'goal per category' instead\n",
    "                       'duration_bins', #want to use actual values instead, using duration_days\n",
    "                       'launch_season', 'deadline_season', #gotten dummies \n",
    "                       #'duration_bins_coded', #dropped the whole encoding code \n",
    "                       ]\n",
    "# keeping the already dummied ones obviously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ab352a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['usd_goal_real', 'duration_days', 'target', 'goal_per_category',\n",
       "       'sl__Spring', 'sl__Summer', 'sl__Winter', 'sd__Spring', 'sd__Summer',\n",
       "       'sd__Winter', 'cat__Creative', 'cat__Entertainment', 'cat__Other',\n",
       "       'cat__Tech', 'co__Europe', 'co__North America', 'co__Oceania'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_to_scale = dff.drop(columns=columns_to_softdrop, axis=1)\n",
    "display(df_to_scale.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2708a4",
   "metadata": {},
   "source": [
    "Scale the remaining numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1c49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df before (293019, 17)\n",
      "X_train shape (205113, 16)\n",
      "X_test shape (87906, 16)\n",
      "y_train shape (205113,)\n",
      "y_test shape (87906,)\n"
     ]
    }
   ],
   "source": [
    "#first, create our dfs \n",
    "X = df_to_scale.drop(columns=['target'])\n",
    "y = df_to_scale['target']\n",
    "#get train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)\n",
    "print(\"Df before\", df_to_scale.shape)\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadd9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the columns to standardise \n",
    "col_scale = ['usd_goal_real',\n",
    "             'duration_days',\n",
    "             'goal_per_category',\n",
    "             ]\n",
    "\n",
    "#instantiate\n",
    "scaler = StandardScaler()\n",
    "#scale \n",
    "X_train_scaled = scaler.fit_transform(X_train[col_scale])\n",
    "X_test_scaled = scaler.fit_transform(X_test[col_scale])\n",
    "#make it a df again\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_scaled,\n",
    "    columns=col_scale,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    X_test_scaled,\n",
    "    columns=col_scale,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147c4022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the original axes again\n",
    "X_train = X_train.drop(col_scale, axis=1)\n",
    "X_test = X_test.drop(col_scale, axis=1)\n",
    "#and check if everything's still in order \n",
    "X_train.index.equals(X_train_scaled.index)\n",
    "X_test.index.equals(X_test_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6535db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dff shape (293019, 27)\n",
      "X_train shape (205113, 3)\n",
      "X_test shape (87906, 3)\n",
      "X_train shape after scaling (205113, 16)\n",
      "X_test shape after scaling (87906, 16)\n",
      "train split head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usd_goal_real</th>\n",
       "      <th>duration_days</th>\n",
       "      <th>goal_per_category</th>\n",
       "      <th>sl__Spring</th>\n",
       "      <th>sl__Summer</th>\n",
       "      <th>sl__Winter</th>\n",
       "      <th>sd__Spring</th>\n",
       "      <th>sd__Summer</th>\n",
       "      <th>sd__Winter</th>\n",
       "      <th>cat__Creative</th>\n",
       "      <th>cat__Entertainment</th>\n",
       "      <th>cat__Other</th>\n",
       "      <th>cat__Tech</th>\n",
       "      <th>co__Europe</th>\n",
       "      <th>co__North America</th>\n",
       "      <th>co__Oceania</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57281</th>\n",
       "      <td>-0.006214</td>\n",
       "      <td>-0.310206</td>\n",
       "      <td>-0.844947</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264757</th>\n",
       "      <td>0.020483</td>\n",
       "      <td>0.885107</td>\n",
       "      <td>-0.192760</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28338</th>\n",
       "      <td>-0.032193</td>\n",
       "      <td>-0.947706</td>\n",
       "      <td>1.452947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150240</th>\n",
       "      <td>-0.037251</td>\n",
       "      <td>1.363232</td>\n",
       "      <td>-0.844947</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291618</th>\n",
       "      <td>-0.028165</td>\n",
       "      <td>-0.310206</td>\n",
       "      <td>0.983668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        usd_goal_real  duration_days  goal_per_category  sl__Spring   \n",
       "57281       -0.006214      -0.310206          -0.844947           0  \\\n",
       "264757       0.020483       0.885107          -0.192760           0   \n",
       "28338       -0.032193      -0.947706           1.452947           0   \n",
       "150240      -0.037251       1.363232          -0.844947           0   \n",
       "291618      -0.028165      -0.310206           0.983668           0   \n",
       "\n",
       "        sl__Summer  sl__Winter  sd__Spring  sd__Summer  sd__Winter   \n",
       "57281            0           0           0           0           0  \\\n",
       "264757           1           0           0           0           0   \n",
       "28338            1           0           0           1           0   \n",
       "150240           0           1           0           0           1   \n",
       "291618           0           0           0           0           0   \n",
       "\n",
       "        cat__Creative  cat__Entertainment  cat__Other  cat__Tech  co__Europe   \n",
       "57281               1                   0           0          0           0  \\\n",
       "264757              0                   0           0          0           0   \n",
       "28338               0                   0           1          0           0   \n",
       "150240              1                   0           0          0           0   \n",
       "291618              0                   1           0          0           0   \n",
       "\n",
       "        co__North America  co__Oceania  \n",
       "57281                   1            0  \n",
       "264757                  1            0  \n",
       "28338                   1            0  \n",
       "150240                  1            0  \n",
       "291618                  1            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test split head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usd_goal_real</th>\n",
       "      <th>duration_days</th>\n",
       "      <th>goal_per_category</th>\n",
       "      <th>sl__Spring</th>\n",
       "      <th>sl__Summer</th>\n",
       "      <th>sl__Winter</th>\n",
       "      <th>sd__Spring</th>\n",
       "      <th>sd__Summer</th>\n",
       "      <th>sd__Winter</th>\n",
       "      <th>cat__Creative</th>\n",
       "      <th>cat__Entertainment</th>\n",
       "      <th>cat__Other</th>\n",
       "      <th>cat__Tech</th>\n",
       "      <th>co__Europe</th>\n",
       "      <th>co__North America</th>\n",
       "      <th>co__Oceania</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74198</th>\n",
       "      <td>-0.030944</td>\n",
       "      <td>2.095838</td>\n",
       "      <td>0.988030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57285</th>\n",
       "      <td>-0.019671</td>\n",
       "      <td>2.095838</td>\n",
       "      <td>0.988030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94280</th>\n",
       "      <td>-0.033632</td>\n",
       "      <td>-0.309181</td>\n",
       "      <td>-1.039898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169955</th>\n",
       "      <td>-0.036054</td>\n",
       "      <td>-0.309181</td>\n",
       "      <td>-1.067390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224769</th>\n",
       "      <td>-0.028533</td>\n",
       "      <td>-0.309181</td>\n",
       "      <td>0.154996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        usd_goal_real  duration_days  goal_per_category  sl__Spring   \n",
       "74198       -0.030944       2.095838           0.988030           0  \\\n",
       "57285       -0.019671       2.095838           0.988030           0   \n",
       "94280       -0.033632      -0.309181          -1.039898           0   \n",
       "169955      -0.036054      -0.309181          -1.067390           0   \n",
       "224769      -0.028533      -0.309181           0.154996           0   \n",
       "\n",
       "        sl__Summer  sl__Winter  sd__Spring  sd__Summer  sd__Winter   \n",
       "74198            1           0           0           1           0  \\\n",
       "57285            0           0           0           0           1   \n",
       "94280            1           0           0           1           0   \n",
       "169955           1           0           0           1           0   \n",
       "224769           0           0           0           0           0   \n",
       "\n",
       "        cat__Creative  cat__Entertainment  cat__Other  cat__Tech  co__Europe   \n",
       "74198               0                   1           0          0           0  \\\n",
       "57285               0                   1           0          0           1   \n",
       "94280               1                   0           0          0           0   \n",
       "169955              1                   0           0          0           0   \n",
       "224769              0                   1           0          0           1   \n",
       "\n",
       "        co__North America  co__Oceania  \n",
       "74198                   1            0  \n",
       "57285                   0            0  \n",
       "94280                   1            0  \n",
       "169955                  0            1  \n",
       "224769                  0            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# put it back together again \n",
    "X_train_sp = pd.concat([X_train_scaled, X_train ], axis=1)\n",
    "X_test_sp = pd.concat([X_test_scaled, X_test], axis=1)\n",
    "#and check\n",
    "print(\"Dff shape\", dff.shape)\n",
    "print(\"X_train shape\", X_train_scaled.shape)\n",
    "print(\"X_test shape\", X_test_scaled.shape)\n",
    "print(\"X_train shape after scaling\", X_train_sp.shape)\n",
    "print(\"X_test shape after scaling\", X_test_sp.shape)\n",
    "print(\"train split head:\")\n",
    "display(X_train_sp.head())\n",
    "print(\"test split head:\")\n",
    "display(X_test_sp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf255c3",
   "metadata": {},
   "source": [
    "## Ensemble Method: Weighed Averages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea105cd",
   "metadata": {},
   "source": [
    "##### current feature engineering (CHANGE ME)\n",
    "\n",
    "you can start using the train-test-split: \n",
    "* X_train_sp\n",
    "* X_test_sp\n",
    "* y_train\n",
    "* y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b37806",
   "metadata": {},
   "source": [
    "### Final Features (CHANGE ME)\n",
    "| Column | Data | Decision | Done |\n",
    "|:--------:|:--------:|:--------:|:--------:|\n",
    "|  Category  |  >150 Subcategories   |  included in main category   | ignore |\n",
    "|main_category| 15 categories| checked, makes a difference| use but make even less granular|\n",
    "|Topic|4 clusters: Tech, Entertaiment, Creative, Other||\n",
    "|country|country of project|informative, but too many|reduced to continents|\n",
    "|usd_pledged_real|redundant apparently|same as pledged | ignored|\n",
    "|deadline| unclear! probably stated end date of kickstarter| used |ignored|\n",
    "|state|used as target |removed all but \"success and fail\"|keep and create new column with numerical values|\n",
    "|country|country of project|informative, but too many|reduced to continents|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029a222",
   "metadata": {},
   "source": [
    "### Weighed Average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81fc5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, get our three models \n",
    "model1 = LogisticRegression(C = 0.01, solver= 'lbfgs', random_state = 42)\n",
    "model2 = KNeighborsClassifier(n_neighbors=17, algorithm='auto', metric='minkowski', p=3)\n",
    "model3 = DecisionTreeClassifier(max_depth=3, min_samples_leaf = 1,  min_samples_split = 2, random_state = 42)\n",
    "#model4 = \n",
    "\n",
    "#fit them individually \n",
    "model1.fit(X_train_sp,y_train)\n",
    "model2.fit(X_train_sp,y_train)\n",
    "model3.fit(X_train_sp,y_train)\n",
    "#get individual predictions \n",
    "pred1 = model1.predict_proba(X_test_sp)\n",
    "pred2 = model2.predict_proba(X_test_sp)\n",
    "pred3 = model3.predict_proba(X_test_sp)\n",
    "#returns array len(X_test) with predicted probability for pos / neg values \n",
    "#---- score it: \n",
    "\n",
    "acc1 = accuracy_score(y_train, model1.predict(X_train_sp))\n",
    "acc2 = accuracy_score(y_train, model2.predict(X_train_sp))\n",
    "acc3 = accuracy_score(y_train, model3.predict(X_train_sp))\n",
    "#sum it up: \n",
    "acc_sum = acc1 + acc2 + acc3\n",
    "# now it gets interesting: \n",
    "weight1 = acc1/acc_sum #which proportion of the outcome\n",
    "weight2 = acc2/acc_sum #does each prediction\n",
    "weight3 = acc3/acc_sum #actually have?\n",
    "#then use those weights on the predictions to go for it: \n",
    "finalpred = (pred1*weight1 + pred2*weight2 + pred3*weight3)\n",
    "#and max that again - as it's an array\n",
    "finalpred = np.argmax(finalpred.round(0), axis=1)\n",
    "(y_test == finalpred).sum() / len(finalpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed8134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, get our three models \n",
    "model1 = LogisticRegression(C = 0.01, solver= 'lbfgs', random_state = 42)\n",
    "model2 = KNeighborsClassifier(n_neighbors=17, algorithm='auto', metric='minkowski', p=3)\n",
    "model3 = DecisionTreeClassifier(max_depth=3, min_samples_leaf = 1,  min_samples_split = 2, random_state = 42)\n",
    "#model4 = \n",
    "\n",
    "#fit them individually \n",
    "model1.fit(X_train_sp,y_train)\n",
    "model2.fit(X_train_sp,y_train)\n",
    "model3.fit(X_train_sp,y_train)\n",
    "#get individual predictions \n",
    "pred1 = model1.predict_proba(X_test_sp)\n",
    "pred2 = model2.predict_proba(X_test_sp)\n",
    "pred3 = model3.predict_proba(X_test_sp)\n",
    "#returns array len(X_test) with predicted probability for pos / neg values \n",
    "#---- score it: \n",
    "#f1_1 = f1_score(y_test, pred1, average='weighted')\n",
    "\n",
    "f1_1 = f1_score(y_train, model1.predict(X_train_sp), average='weighted')\n",
    "f1_2 = f1_score(y_train, model2.predict(X_train_sp), average='weighted')\n",
    "f1_3 = accuracy_score(y_train, model3.predict(X_train_sp), average='weighted')\n",
    "#sum it up: \n",
    "f1n_sum = f1_1 + f1_2 + f2_3\n",
    "# now it gets interesting: \n",
    "weight1 = f1_1/f1n_sum #which proportion of the outcome\n",
    "weight2 = f1_2/f1n_sum #does each prediction\n",
    "weight3 = f1_3/f1n_sum #actually have?\n",
    "#then use those weights on the predictions to go for it: \n",
    "finalpred = (pred1*weight1 + pred2*weight2 + pred3*weight3)\n",
    "#and max that again - as it's an array\n",
    "finalpred = np.argmax(finalpred.round(0), axis=1)\n",
    "(y_test == finalpred).sum() / len(finalpred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
