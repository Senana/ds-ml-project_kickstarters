{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial imports \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get the absolute path of the current file/notebook\n",
    "# If using Jupyter, use Path.cwd(). If using a .py script, use Path(__file__).parent\n",
    "curr_dir = Path.cwd()\n",
    "\n",
    "# Calculate the project root (adjust '.parent' count as needed)\n",
    "# If your notebook is in 'project/notebooks/', the root is 1 level up\n",
    "project_root = curr_dir.parent.parent \n",
    "\n",
    "# Add project root to system path so Python can find 'utils'\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project Root added to path: {project_root}\")\n",
    "\n",
    "from utils.feature_engineer_df import build_features \n",
    "\n",
    "#for the scaling and encoding \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#cleanup \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get your data from our utils\n",
    "build_features(\n",
    "    input_path=Path.cwd().resolve().parents[1] / \"data\" / \"cleaned\" / \"kickstarter_cleaned.csv\",\n",
    "    output_path=Path.cwd().resolve().parents[1] / \"data\" / \"feature\" / \"kickstarter_featured.csv\",\n",
    "    raw_path=Path.cwd().resolve().parents[1] / \"data\" / \"raw\" / \"ks-projects-201801.csv\",\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Load Files as DataFrames\n",
    "BASE_DIR = Path.cwd().resolve().parents[1]\n",
    "data_file = BASE_DIR / \"data\" / \"feature\" / \"kickstarter_featured.csv\"\n",
    "\n",
    "filepath = Path(data_file)\n",
    "\n",
    "df = pd.read_csv(filepath, encoding='latin-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns to \"hard drop\" from feature engineering dataframe\n",
    "columns_to_drop = ['id', #irrelevant\n",
    "                   'main_category', #substituted in a satisfactory way\n",
    "                   'deadline', 'launched', #created new categories \n",
    "                   'backers', 'usd_pledged_real', 'usd_pledged_bins', 'backers_per_pledged', 'backer_pledged_bins', 'pledged_per_category', #everything to do with \"future information\"\n",
    "                   'launched_year', 'deadline_year', #info about the past and not seasonal\n",
    "                   ]\n",
    "# drop them\n",
    "dfc = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sl = pd.get_dummies(dfc['launch_season'], prefix = 'sl_', drop_first=True, dtype=int)\n",
    "df_sd = pd.get_dummies(dfc['deadline_season'], prefix = 'sd_', drop_first=True, dtype=int)\n",
    "df_cat = pd.get_dummies(dfc['main_category_grouped'], prefix = 'cat_', drop_first=True, dtype=int)\n",
    "df_co = pd.get_dummies(dfc['continent'], prefix = 'co_', drop_first=True, dtype=int)\n",
    "df_ugb = pd.get_dummies(dfc['usd_goal_bins'], prefix = 'ugb_', drop_first=True, dtype=int)\n",
    "df_cgp = pd.get_dummies(dfc['category_goal_percentile'], prefix = 'cgp_', drop_first=True, dtype=int)\n",
    "df_db = pd.get_dummies(dfc['duration_bins'], prefix = 'db_', drop_first=True, dtype=int)\n",
    "#put everything back together again: \n",
    "dff = pd.concat([dfc, df_sl, df_sd, df_cat, df_co, df_ugb, df_db, df_cgp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## General thoughts\n",
    "\n",
    "For this prediction project I am doing binary classification: \n",
    "* Positive class = successful\n",
    "\n",
    "* Negative class = failed\n",
    "\n",
    "Risks of FP: waste of invested time and money\n",
    "\n",
    "Risks of FN: waste of talent and lost of great project and potential\n",
    "\n",
    "* Accuracy - for baseline and generall correctness\n",
    "* Precision - how many TP, highlights the importance of FP\n",
    "* Recall - importance of FN\n",
    "* F1 - balance of Precision and Recall\n",
    "\n",
    "| My goal                               | My metric |\n",
    "| ---------------------------------------------------- | --------------- |\n",
    "| **..to avoid launching weak and doomed projects**  | **Precision**   |\n",
    "| **..to find as many good ideas as possible** | **Recall**      |\n",
    "| **..to balance both**          | **F1-score**    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dff = dff.drop(columns=['main_category_grouped', 'continent', 'launch_season', 'deadline_season', 'category_goal_percentile', 'duration_bins', 'usd_goal_bins', 'country'])\n",
    "dff.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, create our dfs \n",
    "X = dff.drop(columns=['target'])        #independent variable\n",
    "y = dff['target']                       #dependent\n",
    "#get train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)\n",
    "#stratify ensures target classes are balanced in train and test sets\n",
    "print(\"Df before\", dff.shape)\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "| Element   | Shape          | Meaning                                         |\n",
    "| --------- | -------------- | ----------------------------------------------- |\n",
    "| `dff`     | `(293019, 33)` | Original full dataset: 293,019 rows, 33 columns |\n",
    "| `X_train` | `(205113, 32)` | 70% of rows used for training, 32 features      |\n",
    "| `X_test`  | `(87906, 32)`  | 30% of rows for testing, same 32 features       |\n",
    "| `y_train` | `(205113,)`    | Target values for training set                  |\n",
    "| `y_test`  | `(87906,)`     | Target values for test set                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Uses a stratified split (to preserve class distribution). Output confirms dimensions before and after the split:\n",
    "\n",
    "Df before (293019, 33)\n",
    "\n",
    "* X_train shape (205113, 32)\n",
    "* X_test shape (87906, 32)\n",
    "* y_train shape (205113,)\n",
    "* y_test shape (87906,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#training my model\n",
    "clf = DecisionTreeClassifier(random_state=42)   #clf = a variable name for the model (short for classifier)    \n",
    "#random state = every time I run the code, I get the same tree structure and results\n",
    "clf.fit(X_train, y_train) #this is where the model starts to learn\n",
    "#X_train: the input features (32 columns) and y_train: the correct labels (success / failure)  - after this step the model is trained\n",
    "\n",
    "#predict\n",
    "y_pred = clf.predict(X_test)        #The trained model is now used on unseen data (X_test)\n",
    "\n",
    "#A DT is created, I trained it on historical data and used it to predict whether new projects would succeed or fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional lore: it removes the first three numbers after the zero\n",
    "\n",
    "#acc_score = accuracy_score(y_test, y_pred)\n",
    "#prec_score = precision_score(y_test, y_pred)\n",
    "#rec_score = recall_score(y_test, y_pred)\n",
    "#f_score = f1_score(y_test, y_pred)\n",
    "#print(f\"Accuracy score: {acc_score:.3f}\")\n",
    "#print(f\"Precision score: {prec_score:.3f}\")\n",
    "#print(f\"Recall score: {rec_score:.3f}\")\n",
    "#print(f\"F1 score: {f_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline performance doesn't look great\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search cv does his best to find the best combination of hyperparameters by testing different options through cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1_score': 'f1'\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],                          #limit the depth of the tree, prevents overfitting if too deep\n",
    "    'min_samples_split': [2, 5, 10],                        #minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]                           #minimum samples at a leaf node, helps smooth predictions and reduce noise\n",
    "}                                                           #GridSearchCV will try all combinations of these values\n",
    "# Set up the grid search\n",
    "grid_accuracy = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),      #base estimator (for DT)\n",
    "    param_grid=param_grid,                                  #search space\n",
    "    scoring=scoring,                                        #multiple metrics\n",
    "    refit='accuracy',                                       #selects the model with the highest accuracy\n",
    "    cv=5,                                                   #5-fold cross-validation\n",
    "    n_jobs=-1                                               #Use all CPU cores for faster computation\n",
    ")\n",
    "\n",
    "grid_recall = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),      \n",
    "    param_grid=param_grid,                                     \n",
    "    scoring=scoring,                                      \n",
    "    refit='recall',                                         #useful when missing successes is costly           \n",
    "    cv=5,                                              \n",
    "    n_jobs=-1                                                  \n",
    ")\n",
    "\n",
    "grid_precision = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),      \n",
    "    param_grid=param_grid,                                    \n",
    "    scoring=scoring,                                     \n",
    "    refit='precision',                                      #useful when false positives are costly                                   \n",
    "    cv=5,                                                 \n",
    "    n_jobs=-1                                                  \n",
    ")\n",
    "\n",
    "grid_f1 = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),      \n",
    "    param_grid=param_grid,                                     \n",
    "    scoring=scoring,                                        \n",
    "    refit='f1_score',                                       #often best for imbalanced datasets                                     \n",
    "    cv=5,                                                 \n",
    "    n_jobs=-1                                                   \n",
    ")\n",
    "\n",
    "#train and evaluate the model on each fold\n",
    "#Try every combination of the parameters\n",
    "#Select the best one based on metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## cv=5: Cross-Validation\n",
    "\n",
    "cv=5 tells GridSearchCV to use 5-fold cross-validation.\n",
    "\n",
    "Cross-validation is a technique for evaluating how well the model generalizes to new, unseen data.\n",
    "\n",
    "The training data is split into 5 equal parts (folds). The model is trained on 4 folds and tested on the remaining fold. This process repeats 5 times, each time using a different fold as the validation set. The results (accuracy, recall) are averaged to give a more reliable estimate of model performance. This avoids the risk of getting lucky or unlucky with a single train/test split.\n",
    "\n",
    "- Why it matters:\n",
    "\n",
    "Gives a more robust evaluation than a single split. Helps avoid overfitting or underfitting during hyperparameter tuning.\n",
    "\n",
    "## n_jobs=-1: CPU Core Usage\n",
    "\n",
    "n_jobs=-1 tells GridSearchCV to use all available CPU cores to run faster. \n",
    "\n",
    "Computer's processor (CPU) has multiple cores. Each core can run a task in parallel with others.\n",
    "\n",
    "- Why it matters:\n",
    "\n",
    "GridSearchCV needs to train many models with different hyperparameters. Running these one-by-one is slow. With n_jobs=-1, Python parallelizes the work: Trains multiple models at the same time. Saves a lot of time, especially on large datasets\n",
    "\n",
    "Setting n_jobs=-1 is a best practice when tuning models with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_accuracy.fit(X_train, y_train)\n",
    "grid_precision.fit(X_train, y_train)\n",
    "grid_recall.fit(X_train, y_train)\n",
    "grid_f1.fit(X_train, y_train)\n",
    "print(\"Best Params:\", grid_accuracy.best_params_)\n",
    "print(\"Best Params:\", grid_precision.best_params_)\n",
    "print(\"Best Params:\", grid_recall.best_params_)\n",
    "print(\"Best Params:\", grid_f1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Each .fit() call:\n",
    "\n",
    "- Runs a grid search over all combinations of hyperparameters.\n",
    "\n",
    "- Uses cross-validation (5 folds).\n",
    "\n",
    "- Evaluates each model using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "- Selects the best model based on the metric I choose.\n",
    "\n",
    "- Stores the best model in .best_estimator_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "If the goal is:\n",
    "\n",
    "- Reliability of success predictions → Accuracy/precision model.\n",
    "\n",
    "- Catching all successes → Recall model.\n",
    "\n",
    "- Balance → F1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the best model found\n",
    "best_accuracy_clf = grid_accuracy.best_estimator_\n",
    "best_precision_clf = grid_precision.best_estimator_\n",
    "best_recall_clf = grid_recall.best_estimator_\n",
    "best_f1_clf = grid_f1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_acc = best_accuracy_clf.predict(X_test)\n",
    "y_pred_prec = best_precision_clf.predict(X_test)\n",
    "y_pred_rec = best_recall_clf.predict(X_test)\n",
    "y_pred_f1 = best_f1_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Model:\", accuracy_score(y_test, y_pred_acc))\n",
    "print(\"Precision Model:\", precision_score(y_test, y_pred_prec))\n",
    "print(\"Recall Model:\", recall_score(y_test, y_pred_rec))\n",
    "print(\"F1 Model:\", f1_score(y_test, y_pred_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_f1_clf, filled=True, feature_names=X_train.columns, max_depth=4, fontsize=10)\n",
    "plt.title(\"Simplified Decision Tree Structure (First 5 Levels)\")\n",
    "plt.show()\n",
    "\n",
    "#each box is a decision node or a leaf:\n",
    "#split condition (usd_goal_real <= 15002.375)\n",
    "#gini: gini impurity (0 = pure, 0.5 = most impure)\n",
    "#samples: number of data points that reached this node\n",
    "#value = [class_0, class_1]: Count of samples per class (failed, successful)\n",
    "#color: indicates purity and dominant class (blueish = more class 1, orange = more class 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- usd_goal_real:\t    \n",
    "\n",
    "most influential split feature — low goals lead to better success chances\n",
    "\n",
    "- duration_days:\t    \n",
    "\n",
    "shorter campaigns among low-goal projects increase likelihood of success\n",
    "\n",
    "- db__4 weeks:\t        \n",
    "\n",
    "duration bin split matters for medium-to-high goal campaigns\n",
    "\n",
    "- cat__Entertainment:\t\n",
    "\n",
    "certain categories perform differently — entertainment slightly more successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrices\n",
    "       #Predicted\n",
    "            #0     1\n",
    "#Actual  0 |  TN |  FP |\n",
    "        #1 |  FN |  TP |\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define a helper to show the confusion matrix\n",
    "def show_conf_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#display for each optimized model\n",
    "show_conf_matrix(y_test, y_pred_acc, \"Confusion Matrix - Accuracy Optimized\")\n",
    "show_conf_matrix(y_test, y_pred_prec, \"Confusion Matrix - Precision Optimized\")\n",
    "show_conf_matrix(y_test, y_pred_rec, \"Confusion Matrix - Recall Optimized\")\n",
    "show_conf_matrix(y_test, y_pred_f1, \"Confusion Matrix - F1 Optimized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "* More TP: better at finding successful projects\n",
    "\n",
    "* More FP: more false alarms (bad projects that looked good)\n",
    "\n",
    "* More FN: missed opportunities (good projects predicted as failures)\n",
    "\n",
    "- FP ↓ = Precision ↑\n",
    "\n",
    "- FN ↓ = Recall ↑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance plot (from best model)\n",
    "#here I assume best_f1_clf for demo\n",
    "importances = best_f1_clf.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "#sort and plot\n",
    "feat_imp = pd.Series(importances, index=features).sort_values(ascending=True).tail(15)\n",
    "feat_imp.plot(kind='barh', figsize=(10, 6))\n",
    "plt.title(\"Top 15 Feature Importances (F1-Optimized Decision Tree)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#replace best_f1_clf with another model to analyze a different one.\n",
    "\n",
    "#PRECISION\n",
    "importances = best_precision_clf.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "#sort and plot\n",
    "feat_imp = pd.Series(importances, index=features).sort_values(ascending=True).tail(15)\n",
    "feat_imp.plot(kind='barh', figsize=(10, 6))\n",
    "plt.title(\"Top 15 Feature Importances (Precision-Optimized Decision Tree)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#RECALL\n",
    "importances = best_recall_clf.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "#sort and plot\n",
    "feat_imp = pd.Series(importances, index=features).sort_values(ascending=True).tail(15)\n",
    "feat_imp.plot(kind='barh', figsize=(10, 6))\n",
    "plt.title(\"Top 15 Feature Importances (Recall-Optimized Decision Tree)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#ACCURACY\n",
    "importances = best_accuracy_clf.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "#sort and plot\n",
    "feat_imp = pd.Series(importances, index=features).sort_values(ascending=True).tail(15)\n",
    "feat_imp.plot(kind='barh', figsize=(10, 6))\n",
    "plt.title(\"Top 15 Feature Importances (Accuracy-Optimized Decision Tree)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "* feature_importances: gives the relative importance of each feature used by the tree in making splits.\n",
    "\n",
    "* sort_values(ascending=True).tail(15): picks the 15 most important features, sorted from least to most.\n",
    "\n",
    "* barh: horizontal bar chart makes it easy to compare feature impact.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "* Features like usd_goal_real, duration_days or specific one-hot-encoded season/category features seemed to be important.\n",
    "\n",
    "* Differences across models (F1 vs. Precision) can tell which features contribute to recall vs. predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## HOW CAN I IMPROVE MY MODEL?\n",
    "\n",
    "1. Class weighting \n",
    "\n",
    "- for class imbalance\n",
    "\n",
    "2. Reducing Features and trying both reduced and not-reduced features for modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 1. Class imbalance \n",
    "\n",
    " class_weight='balanced' \n",
    " \n",
    " unbalanced classes can hurt the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_balanced = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Balanced Decision Tree Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_balanced))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_balanced))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_balanced))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_balanced, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Balanced Decision Tree\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Note\n",
    "Class_weight for balancing made my model worse, it is an incorrect way to improve my model. \n",
    "\n",
    "* Apparently that happens when classes are not severely imbalanced\n",
    "\n",
    "* The decision boundary becomes distorted\n",
    "\n",
    "Let's try something different.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 2. Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(best_f1_clf, prefit=True)\n",
    "X_train_reduced = selector.transform(X_train)\n",
    "X_test_reduced = selector.transform(X_test)\n",
    "\n",
    "print(\"Original feature count:\", X_train.shape[1])\n",
    "print(\"Reduced feature count:\", X_train_reduced.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_reduced = DecisionTreeClassifier(random_state=42)\n",
    "clf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_pred_reduced = clf_reduced.predict(X_test_reduced)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_reduced))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_reduced))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_reduced))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_reduced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(selected_features.tolist())           #names of Kept Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Note:\n",
    "This model works slightly worse, by removing less important or noisy features I might have oversimplified my model.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "What if I apply RF and XGBoost on reduced features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_pred_rf = rf_clf.predict(X_test_reduced)\n",
    "\n",
    "print(\"Random Forest on Reduced Features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_clf.predict(X_test_reduced)\n",
    "\n",
    "print(\"XGBoost on Reduced Features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_xgb))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_xgb))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "What if on not-reduced, origital train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest on Not-Reduced Features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "#xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "#y_pred_xgb = xgb_clf.predict(X_test_reduced)\n",
    "\n",
    "#print(\"XGBoost on Not_Reduced Features:\")\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "#print(\"Precision:\", precision_score(y_test, y_pred_xgb))\n",
    "#print(\"Recall:\", recall_score(y_test, y_pred_xgb))\n",
    "#print(\"F1 Score:\", f1_score(y_test, y_pred_xgb))\n",
    "\n",
    "#ValueError: feature_names must be string, and may not contain [, ] or\n",
    "#the line xgb_clf.fit(X_train, y_train)  is the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Final thoughts on my models:\n",
    "\n",
    "Why did other models work slightly worse?\n",
    "\n",
    "I explored the following model improvements:\n",
    "\n",
    "- Class weighting\n",
    "\n",
    "- Feature selection\n",
    "\n",
    "- Ensemble models (Random Forest, XGBoost) - with and without reduced features\n",
    "\n",
    "\n",
    "None of them surpassed the performance of the tuned decision tree in the beginning:\n",
    "\n",
    "* Perhaps the feature space is relatively simple\n",
    "\n",
    "* Decision trees already captured the important patterns\n",
    "\n",
    "* Overcomplicating the model led to overfitting or minimal gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Careful feature engineering and grid search might matter more than model choice.\n",
    "\n",
    "Model achieved a good balance between detecting successes and avoiding false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## As a reminder, my first model (not the baseline!) results:\n",
    "Accuracy Model: 0.64646326758128\n",
    "\n",
    "Precision Model: 0.6177634130575307\n",
    "\n",
    "Recall Model: 0.6988080722621743\n",
    "\n",
    "F1 Model: 0.6193731942392096"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
