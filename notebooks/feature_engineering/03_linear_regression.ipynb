{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "This notebook builds a time-safe Logistic Regression model to predict project success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "# Import feature engineering function from utils\n",
    "from utils.feature_engineer_df import engineer_launch_time_features\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load Prepared Data\n",
    "\n",
    "We'll recreate the preprocessing steps from `02_feature_engineering.ipynb` to get our train/test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the project root\n",
    "curr_dir = Path.cwd()\n",
    "project_root = curr_dir.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Load cleaned data\n",
    "cleaned_path = project_root / \"data\" / \"cleaned\" / \"kickstarter_cleaned.csv\"\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(cleaned_path, encoding='latin-1', low_memory=False)\n",
    "print(f\"Loaded cleaned dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Convert launched to datetime\n",
    "df['launched'] = pd.to_datetime(df['launched'], errors='coerce')\n",
    "df['deadline'] = pd.to_datetime(df['deadline'], errors='coerce')\n",
    "\n",
    "# Filter only on launch-time valid fields (NOT pledged/backers)\n",
    "df = df[df['usd_goal_real'] > 0].copy()\n",
    "df = df[df['launched'].notna()].copy()\n",
    "df = df[df['deadline'].notna()].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering (goal > 0, valid dates): {df.shape[0]:,} rows\")\n",
    "print(\"Target distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Success rate: {df['target'].mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Feature Engineering - Launch-Time Features\n",
    "\n",
    "We'll create features that are all available at launch time:\n",
    "- Time features (weekday, month)\n",
    "- Goal context (log goal, goal per day)\n",
    "- Category and continent features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We'll use the `engineer_launch_time_features` function from utils to create all launch-time features. This ensures consistency with other notebooks and maintains a single source of truth for feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create all launch-time features BEFORE time split\n",
    "# engineer_launch_time_features creates all features that are available at launch time:\n",
    "# - main_category_grouped, continent\n",
    "# - launched_month, deadline_month, launched_weekday\n",
    "# - log_usd_goal, goal_per_day\n",
    "# These don't depend on aggregates, so it is safe to create before splitting\n",
    "\n",
    "df = engineer_launch_time_features(df)\n",
    "\n",
    "print(\"Created launch-time features using engineer_launch_time_features:\")\n",
    "print(f\"  - main_category_grouped: {df['main_category_grouped'].nunique()} categories\")\n",
    "print(f\"  - continent: {df['continent'].nunique()} categories\")\n",
    "print(f\"  - launched_month: {df['launched_month'].nunique()} months (1-12)\")\n",
    "print(f\"  - launched_weekday: {df['launched_weekday'].nunique()} weekdays (0=Monday, 6=Sunday)\")\n",
    "print(f\"  - log_usd_goal: created\")\n",
    "print(f\"  - goal_per_day: created\")\n",
    "\n",
    "# Step 2: Time based split\n",
    "# Sort by launch date\n",
    "df_sorted = df.sort_values('launched').copy()\n",
    "\n",
    "# Use 70% earliest projects for training, 30% latest for testing\n",
    "split_idx = int(len(df_sorted) * 0.7)\n",
    "df_train_raw = df_sorted.iloc[:split_idx].copy()\n",
    "df_test_raw = df_sorted.iloc[split_idx:].copy()\n",
    "\n",
    "print(\"\\nTime-based split:\")\n",
    "print(f\"Train: {df_train_raw['launched'].min()} to {df_train_raw['launched'].max()}\")\n",
    "print(f\"Test:  {df_test_raw['launched'].min()} to {df_test_raw['launched'].max()}\")\n",
    "print(f\"\\nTrain size: {len(df_train_raw):,} ({len(df_train_raw)/len(df_sorted):.1%})\")\n",
    "print(f\"Test size:  {len(df_test_raw):,} ({len(df_test_raw)/len(df_sorted):.1%})\")\n",
    "print(f\"\\nTrain success rate: {df_train_raw['target'].mean():.2%}\")\n",
    "print(f\"Test success rate:  {df_test_raw['target'].mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare features\n",
    "# Note: log_usd_goal and goal_per_day were already created by engineer_launch_time_features\n",
    "# before the split, so they're already available in df_train_raw and df_test_raw\n",
    "\n",
    "# Columns to drop (keep 'launched' temporarily for train/val split)\n",
    "columns_to_drop = [\n",
    "    'id', 'main_category', 'deadline',\n",
    "    'backers', 'usd_pledged_real', 'usd_pledged_bins', 'backers_per_pledged', \n",
    "    'backer_pledged_bins', 'pledged_per_category',\n",
    "    'launched_year', 'deadline_year', 'usd_goal_real',\n",
    "    'usd_goal_bins', 'category_goal_percentile', 'duration_bins',\n",
    "    'country',\n",
    "    'deadline_month',\n",
    "]\n",
    "\n",
    "# Prepare feature sets\n",
    "df_train_clean = df_train_raw.drop(columns=[col for col in columns_to_drop if col in df_train_raw.columns])\n",
    "df_test_clean = df_test_raw.drop(columns=[col for col in columns_to_drop if col in df_test_raw.columns])\n",
    "\n",
    "print(\"Features after cleaning:\")\n",
    "print(f\"Train: {df_train_clean.shape[1]} columns\")\n",
    "print(f\"Test:  {df_test_clean.shape[1]} columns\")\n",
    "print(f\"\\nFeature columns: {[c for c in df_train_clean.columns if c != 'target']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split train into train/val for threshold optimization\n",
    "# This prevents overfitting threshold choice to training data\n",
    "df_train_clean_sorted = df_train_clean.sort_values('launched').copy()\n",
    "val_split_idx = int(len(df_train_clean_sorted) * 0.8)  # 80% train, 20% val\n",
    "\n",
    "df_train_final = df_train_clean_sorted.iloc[:val_split_idx].copy()\n",
    "df_val_final = df_train_clean_sorted.iloc[val_split_idx:].copy()\n",
    "\n",
    "print(\"Train/Val split (time-based):\")\n",
    "print(f\"Train: {df_train_final['launched'].min()} to {df_train_final['launched'].max()}\")\n",
    "print(f\"Val:    {df_val_final['launched'].min()} to {df_val_final['launched'].max()}\")\n",
    "print(f\"\\nTrain size: {len(df_train_final):,} ({len(df_train_final)/len(df_train_clean_sorted):.1%})\")\n",
    "print(f\"Val size:   {len(df_val_final):,} ({len(df_val_final)/len(df_train_clean_sorted):.1%})\")\n",
    "print(f\"Test size:  {len(df_test_clean):,}\")\n",
    "\n",
    "# Now drop 'launched' and separate X and y\n",
    "X_train = df_train_final.drop(columns=['target', 'launched'])\n",
    "y_train = df_train_final['target']\n",
    "X_val = df_val_final.drop(columns=['target', 'launched'])\n",
    "y_val = df_val_final['target']\n",
    "X_test = df_test_clean.drop(columns=['target', 'launched'])\n",
    "y_test = df_test_clean['target']\n",
    "\n",
    "print(\"\\nFinal feature matrices:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(\"\\nTarget distributions:\")\n",
    "print(f\"Train: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Val:   {y_val.value_counts().to_dict()}\")\n",
    "print(f\"Test:  {y_test.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create preprocessing pipeline using ColumnTransformer\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = ['log_usd_goal', 'duration_days', 'goal_per_day', 'launched_weekday']\n",
    "categorical_features = ['launched_month', 'main_category_grouped', 'continent']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any other columns\n",
    ")\n",
    "\n",
    "# Fit on training data and transform all sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names\n",
    "numeric_names = numeric_features\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "cat_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "feature_names = list(numeric_names) + list(cat_names)\n",
    "\n",
    "# Convert to DataFrames for easier inspection\n",
    "X_train_final = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "X_val_final = pd.DataFrame(X_val_processed, columns=feature_names, index=X_val.index)\n",
    "X_test_final = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "print(f\"X_train_final: {X_train_final.shape}\")\n",
    "print(f\"X_val_final:   {X_val_final.shape}\")\n",
    "print(f\"X_test_final:  {X_test_final.shape}\")\n",
    "print(f\"\\nFeature names: {feature_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Build and Train Logistic Regression Model\n",
    "\n",
    "We'll use Logistic Regression, which is a linear model appropriate for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune regularization parameter C\n",
    "# Smaller C = stronger regularization\n",
    "# Wider grid: LR can be very sensitive to regularization strength with one-hot features\n",
    "param_grid = {\n",
    "    'C': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "base_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Use TimeSeriesSplit for CV to respect temporal ordering within training set\n",
    "# This prevents mixing earlier and later projects in CV folds\n",
    "cv_splitter = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Tuning regularization parameter C...\")\n",
    "print(\"Using TimeSeriesSplit for cross-validation (respects temporal ordering)\")\n",
    "grid_search = GridSearchCV(\n",
    "    base_model,\n",
    "    param_grid,\n",
    "    cv=cv_splitter,  # Time-aware CV instead of random K-fold\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(f\"\\nBest C: {grid_search.best_params_['C']}\")\n",
    "print(f\"Best CV ROC-AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "lr_model = grid_search.best_estimator_\n",
    "\n",
    "# Compare base_model defaults vs best parameters\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Parameter Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBase Model (default parameters):\")\n",
    "print(f\"  C: {base_model.get_params()['C']} (default)\")\n",
    "print(f\"  solver: {base_model.get_params()['solver']} (default)\")\n",
    "print(f\"  max_iter: {base_model.get_params()['max_iter']}\")\n",
    "print(f\"  random_state: {base_model.get_params()['random_state']}\")\n",
    "\n",
    "print(\"\\nBest Model (from GridSearchCV):\")\n",
    "print(f\"  C: {grid_search.best_params_['C']}\")\n",
    "print(f\"  solver: {grid_search.best_params_['solver']}\")\n",
    "print(f\"  max_iter: {lr_model.get_params()['max_iter']}\")\n",
    "print(f\"  random_state: {lr_model.get_params()['random_state']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Difference:\")\n",
    "default_c = base_model.get_params()['C']\n",
    "best_c = grid_search.best_params_['C']\n",
    "print(f\"  C parameter: {default_c} (default) → {best_c} (optimized)\")\n",
    "if best_c < default_c:\n",
    "    print(f\"  → The optimized model uses STRONGER regularization (smaller C = more regularization)\")\n",
    "elif best_c > default_c:\n",
    "    print(f\"  → The optimized model uses WEAKER regularization (larger C = less regularization)\")\n",
    "else:\n",
    "    print(f\"  → Same regularization strength\")\n",
    "print(f\"  Solver: Same ({grid_search.best_params_['solver']})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare base_model defaults vs best parameters\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Parameter Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBase Model (default parameters):\")\n",
    "print(f\"  C: {base_model.get_params()['C']} (default)\")\n",
    "print(f\"  solver: {base_model.get_params()['solver']} (default)\")\n",
    "print(f\"  max_iter: {base_model.get_params()['max_iter']}\")\n",
    "print(f\"  random_state: {base_model.get_params()['random_state']}\")\n",
    "\n",
    "print(\"\\nBest Model (from GridSearchCV):\")\n",
    "print(f\"  C: {grid_search.best_params_['C']}\")\n",
    "print(f\"  solver: {grid_search.best_params_['solver']}\")\n",
    "print(f\"  max_iter: {lr_model.get_params()['max_iter']}\")\n",
    "print(f\"  random_state: {lr_model.get_params()['random_state']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Difference:\")\n",
    "default_c = base_model.get_params()['C']\n",
    "best_c = grid_search.best_params_['C']\n",
    "print(f\"  C parameter: {default_c} (default) → {best_c} (optimized)\")\n",
    "if best_c < default_c:\n",
    "    print(f\"  → The optimized model uses STRONGER regularization (smaller C = more regularization)\")\n",
    "elif best_c > default_c:\n",
    "    print(f\"  → The optimized model uses WEAKER regularization (larger C = less regularization)\")\n",
    "else:\n",
    "    print(f\"  → Same regularization strength\")\n",
    "print(f\"  Solver: Same ({grid_search.best_params_['solver']})\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base_model defaults vs best parameters\n",
    "print(\"Parameter Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nBase Model (default parameters):\")\n",
    "print(f\"  C: {base_model.get_params()['C']} (default)\")\n",
    "print(f\"  solver: {base_model.get_params()['solver']} (default)\")\n",
    "print(f\"  max_iter: {base_model.get_params()['max_iter']}\")\n",
    "print(f\"  random_state: {base_model.get_params()['random_state']}\")\n",
    "\n",
    "print(\"\\nBest Model (from GridSearchCV):\")\n",
    "print(f\"  C: {grid_search.best_params_['C']}\")\n",
    "print(f\"  solver: {grid_search.best_params_['solver']}\")\n",
    "print(f\"  max_iter: {lr_model.get_params()['max_iter']}\")\n",
    "print(f\"  random_state: {lr_model.get_params()['random_state']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Key Difference:\")\n",
    "print(f\"  C parameter: {base_model.get_params()['C']} (default) → {grid_search.best_params_['C']} (optimized)\")\n",
    "print(f\"  This means the optimized model uses {'stronger' if grid_search.best_params_['C'] < base_model.get_params()['C'] else 'weaker'} regularization\")\n",
    "print(f\"  Solver: Same ({grid_search.best_params_['solver']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities (threshold-independent metrics)\n",
    "y_train_proba = lr_model.predict_proba(X_train_final)[:, 1]\n",
    "y_val_proba = lr_model.predict_proba(X_val_final)[:, 1]\n",
    "y_test_proba = lr_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# ROC-AUC (threshold-independent)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_proba)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(\"Threshold-Independent Metrics:\")\n",
    "print(f\"Train ROC-AUC: {train_roc_auc:.4f}\")\n",
    "print(f\"Val ROC-AUC:   {val_roc_auc:.4f}\")\n",
    "print(f\"Test ROC-AUC:  {test_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize threshold on VALIDATION set (not training - prevents overfitting)\n",
    "# Find threshold that maximizes F1 score\n",
    "# Note: F1 assumes false positives and false negatives are equally costly.\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "f1_scores = np.nan_to_num(f1_scores)\n",
    "\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[best_threshold_idx]\n",
    "\n",
    "print(f\"\\nOptimal threshold (maximizing F1 on VALIDATION): {optimal_threshold:.3f}\")\n",
    "print(\"Default threshold: 0.5\")\n",
    "print(\"\\nNote: This threshold optimizes F1 on validation set (assumes symmetric costs).\")\n",
    "\n",
    "# Apply optimal threshold to all sets\n",
    "y_train_pred_opt = (y_train_proba >= optimal_threshold).astype(int)\n",
    "y_val_pred_opt = (y_val_proba >= optimal_threshold).astype(int)\n",
    "y_test_pred_opt = (y_test_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Also get default threshold predictions for comparison\n",
    "y_train_pred_default = (y_train_proba >= 0.5).astype(int)\n",
    "y_val_pred_default = (y_val_proba >= 0.5).astype(int)\n",
    "y_test_pred_default = (y_test_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics at both thresholds\n",
    "metrics_opt = {\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_train, y_train_pred_opt),\n",
    "        accuracy_score(y_val, y_val_pred_opt),\n",
    "        accuracy_score(y_test, y_test_pred_opt)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_train, y_train_pred_opt),\n",
    "        precision_score(y_val, y_val_pred_opt),\n",
    "        precision_score(y_test, y_test_pred_opt)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_train, y_train_pred_opt),\n",
    "        recall_score(y_val, y_val_pred_opt),\n",
    "        recall_score(y_test, y_test_pred_opt)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_train, y_train_pred_opt),\n",
    "        f1_score(y_val, y_val_pred_opt),\n",
    "        f1_score(y_test, y_test_pred_opt)\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics_default = {\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_train, y_train_pred_default),\n",
    "        accuracy_score(y_val, y_val_pred_default),\n",
    "        accuracy_score(y_test, y_test_pred_default)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_train, y_train_pred_default),\n",
    "        precision_score(y_val, y_val_pred_default),\n",
    "        precision_score(y_test, y_test_pred_default)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_train, y_train_pred_default),\n",
    "        recall_score(y_val, y_val_pred_default),\n",
    "        recall_score(y_test, y_test_pred_default)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_train, y_train_pred_default),\n",
    "        f1_score(y_val, y_val_pred_default),\n",
    "        f1_score(y_test, y_test_pred_default)\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Train (Optimal)': [m[0] for m in metrics_opt.values()],\n",
    "    'Val (Optimal)': [m[1] for m in metrics_opt.values()],\n",
    "    'Test (Optimal)': [m[2] for m in metrics_opt.values()],\n",
    "    'Train (Default 0.5)': [m[0] for m in metrics_default.values()],\n",
    "    'Val (Default 0.5)': [m[1] for m in metrics_default.values()],\n",
    "    'Test (Default 0.5)': [m[2] for m in metrics_default.values()]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix at optimal threshold\n",
    "cm_opt = confusion_matrix(y_test, y_test_pred_opt)\n",
    "cm_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Failed', 'Successful'],\n",
    "            yticklabels=['Failed', 'Successful'])\n",
    "axes[0].set_title(f'Confusion Matrix - Optimal Threshold ({optimal_threshold:.3f})')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Failed', 'Successful'],\n",
    "            yticklabels=['Failed', 'Successful'])\n",
    "axes[1].set_title('Confusion Matrix - Default Threshold (0.5)')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix breakdown\n",
    "tn, fp, fn, tp = cm_opt.ravel()\n",
    "print(\"\\nConfusion Matrix Breakdown (Optimal Threshold):\")\n",
    "print(f\"True Negatives (Correctly predicted failures): {tn:,}\")\n",
    "print(f\"False Positives (Predicted success, actually failed): {fp:,}\")\n",
    "print(f\"False Negatives (Predicted failure, actually successful): {fn:,}\")\n",
    "print(f\"True Positives (Correctly predicted successes): {tp:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve and Precision-Recall Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "\n",
    "axes[0].plot(fpr_train, tpr_train, label=f'Train ROC (AUC = {train_roc_auc:.3f})', linestyle='--')\n",
    "axes[0].plot(fpr_val, tpr_val, label=f'Val ROC (AUC = {val_roc_auc:.3f})', linestyle=':')\n",
    "axes[0].plot(fpr_test, tpr_test, label=f'Test ROC (AUC = {test_roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec_train, rec_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "prec_val, rec_val, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "prec_test, rec_test, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "\n",
    "axes[1].plot(rec_train, prec_train, label='Train', linestyle='--')\n",
    "axes[1].plot(rec_val, prec_val, label='Val', linestyle=':')\n",
    "axes[1].plot(rec_test, prec_test, label='Test')\n",
    "axes[1].axhline(y=y_train.mean(), color='k', linestyle='--', label='Baseline (Success Rate)')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Logistic Regression coefficients can help us understand which features are most important for predicting success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort by absolute value of coefficient\n",
    "feature_importance['abs_coefficient'] = feature_importance['coefficient'].abs()\n",
    "feature_importance = feature_importance.sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "top_n = min(20, len(feature_importance))\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features['coefficient']]\n",
    "plt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Coefficient Value (log-odds)')\n",
    "plt.title(f'Top {top_n} Feature Coefficients\\n(Green = Increases success probability, Red = Decreases)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
