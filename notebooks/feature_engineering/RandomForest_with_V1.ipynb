{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## RandomForest \n",
    "Scaling version 1 from 02_feature_engineering\n",
    "used dummies for pure categoricals, kept numericals and scaled them using StandardScaler.\n",
    "\n",
    "#### First Exploration\n",
    "Try a random forest and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial imports \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get the absolute path of the current file/notebook\n",
    "# If using Jupyter, use Path.cwd(). If using a .py script, use Path(__file__).parent\n",
    "curr_dir = Path.cwd()\n",
    "\n",
    "# Calculate the project root (adjust '.parent' count as needed)\n",
    "# If your notebook is in 'project/notebooks/', the root is 1 level up\n",
    "project_root = curr_dir.parent.parent \n",
    "\n",
    "# Add project root to system path so Python can find 'utils'\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project Root added to path: {project_root}\")\n",
    "\n",
    "from utils.feature_engineer_df import build_features \n",
    "\n",
    "#for the scaling and encoding \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#cleanup \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get your data from our utils\n",
    "build_features(\n",
    "    input_path=Path.cwd().resolve().parents[1] / \"data\" / \"cleaned\" / \"kickstarter_cleaned.csv\",\n",
    "    output_path=Path.cwd().resolve().parents[1] / \"data\" / \"feature\" / \"kickstarter_featured.csv\",\n",
    "    raw_path=Path.cwd().resolve().parents[1] / \"data\" / \"raw\" / \"ks-projects-201801.csv\",\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Load Files as DataFrames\n",
    "BASE_DIR = Path.cwd().resolve().parents[1]\n",
    "data_file = BASE_DIR / \"data\" / \"feature\" / \"kickstarter_featured.csv\"\n",
    "\n",
    "filepath = Path(data_file)\n",
    "\n",
    "df = pd.read_csv(filepath, encoding='latin-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns to \"hard drop\" from feature engineering dataframe\n",
    "columns_to_drop = ['id', #irrelevant\n",
    "                   'main_category', #substituted in a satisfactory way\n",
    "                   'deadline', 'launched', #created new categories \n",
    "                   'backers', 'usd_pledged_real', 'usd_pledged_bins', 'backers_per_pledged', 'backer_pledged_bins', 'pledged_per_category', #everything to do with \"future information\"\n",
    "                   'launched_year', 'deadline_year', #info about the past and not seasonal\n",
    "                   ]\n",
    "# drop them\n",
    "dfc = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Get dummies for pure categoricals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#season launched, dropping first as it's multicollinear\n",
    "df_sl = pd.get_dummies(df['launch_season'], prefix = 'sl_', drop_first=True, dtype=int)\n",
    "\n",
    "#season deadline, dropping first\n",
    "df_sd = pd.get_dummies(df['deadline_season'], prefix = 'sd_', drop_first=True, dtype=int)\n",
    "\n",
    "#main category_grouped, dropping first\n",
    "df_cat = pd.get_dummies(df['main_category_grouped'], prefix = 'cat_', drop_first=True, dtype=int)\n",
    "\n",
    "#continent, dropping first\n",
    "df_co = pd.get_dummies(df['continent'], prefix = 'co_', drop_first=True, dtype=int)\n",
    "\n",
    "#put everything back together again: \n",
    "dff = pd.concat([dfc, df_sl, df_sd, df_cat, df_co], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's soft-drop everything we just encoded\n",
    "#commented out all the keepers \n",
    "columns_to_softdrop = ['country', #we kept it for comparison\n",
    "                       # 'usd_goal_real', #right now I want to try scaling actual values \n",
    "                       #'duration_days', #I want to scale these and drop the bins instead for now \n",
    "                       # 'target', (obviously)\n",
    "                       'main_category_grouped', 'continent', #after creating dummies, get rid of these!\n",
    "                        'launched_month', 'deadline_month', #because we have season but might want to look closer\n",
    "                        'usd_goal_bins', #using category_goal_percentile (those two are redundant)\n",
    "                        #'goal_per_category', #it's a polynomial feature - not independent but that's probably ok\n",
    "                       'category_goal_percentile', #it's an orinal bin so keeping 'goal per category' instead\n",
    "                       'duration_bins', #want to use actual values instead, using duration_days\n",
    "                       'launch_season', 'deadline_season', #gotten dummies \n",
    "                       #'duration_bins_coded', #dropped the whole encoding code \n",
    "                       ]\n",
    "# keeping the already dummied ones obviously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_scale = dff.drop(columns=columns_to_softdrop, axis=1)\n",
    "display(df_to_scale.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Scale the remaining numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, create our dfs \n",
    "X = df_to_scale.drop(columns=['target'])\n",
    "y = df_to_scale['target']\n",
    "#get train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)\n",
    "print(\"Df before\", df_to_scale.shape)\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the columns to standardise \n",
    "col_scale = ['usd_goal_real',\n",
    "             'duration_days',\n",
    "             'goal_per_category',\n",
    "             ]\n",
    "\n",
    "#instantiate\n",
    "scaler = StandardScaler()\n",
    "#scale \n",
    "X_train_scaled = scaler.fit_transform(X_train[col_scale])\n",
    "X_test_scaled = scaler.fit_transform(X_test[col_scale])\n",
    "#make it a df again\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_scaled,\n",
    "    columns=col_scale,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    X_test_scaled,\n",
    "    columns=col_scale,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the original axes again\n",
    "X_train = X_train.drop(col_scale, axis=1)\n",
    "X_test = X_test.drop(col_scale, axis=1)\n",
    "#and check if everything's still in order \n",
    "X_train.index.equals(X_train_scaled.index)\n",
    "X_test.index.equals(X_test_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back together again \n",
    "X_train_sp = pd.concat([X_train_scaled, X_train ], axis=1)\n",
    "X_test_sp = pd.concat([X_test_scaled, X_test], axis=1)\n",
    "#and check\n",
    "print(\"Dff shape\", dff.shape)\n",
    "print(\"X_train shape\", X_train_scaled.shape)\n",
    "print(\"X_test shape\", X_test_scaled.shape)\n",
    "print(\"X_train shape after scaling\", X_train_sp.shape)\n",
    "print(\"X_test shape after scaling\", X_test_sp.shape)\n",
    "print(\"train split head:\")\n",
    "display(X_train_sp.head())\n",
    "print(\"test split head:\")\n",
    "display(X_test_sp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Get a baseline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_to_scale.corr()\n",
    "sns.heatmap(data=corr, annot=True, linewidth=0.5, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "using the train-test-split: \n",
    "* X_train_sp\n",
    "* X_test_sp\n",
    "* y_train\n",
    "* y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features for feature importance \n",
    "features = list(X_train_sp.columns)\n",
    "#create a model\n",
    "model = RandomForestClassifier(n_estimators = 50, #start low\n",
    "                               random_state = 42, #just any\n",
    "                               max_features=8, #manually set it to 1/2 dataset \n",
    "                               n_jobs=-1, #use whole CPU\n",
    "                               )\n",
    "#fit the model\n",
    "model.fit(X_train_sp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first description of the model \n",
    "n_nodes = []\n",
    "max_depths = []\n",
    "#create stats \n",
    "for tree in model.estimators_:\n",
    "  n_nodes.append(tree.tree_.node_count) #ask the tree how many nodes it has\n",
    "  max_depths.append(tree.tree_.max_depth)\n",
    "#get averages  \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the original\n",
    "train_predictions = model.predict(X_train_sp)\n",
    "train_probabilites = model.predict_proba(X_train_sp)[:, 1] #get the positives\n",
    "#check with test: \n",
    "test_predictions = model.predict(X_test_sp)\n",
    "test_probabilities = model.predict_proba(X_test_sp)[:, 1]\n",
    "\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, train_probabilites)}')\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, test_probabilities)}')\n",
    "print(f\"Baseline ROC AUC calling everyone successful: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_model = pd.DataFrame({'feature': features,\n",
    "                         'importance': model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "fi_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### one shot at optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter grid \n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 40, 50, 70, 90, 120], #just because I can, fibonacci-style guess\n",
    "    'max_depth': [None] + list(np.arange(3, 21).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None, 8], \n",
    "    'max_leaf_nodes': [20, 100, 1000, 3000, 8000, 20000, 60000, 100000],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "#get an instance to use for random search:\n",
    "estimator = RandomForestClassifier(random_state = 42)\n",
    "#create random search model \n",
    "rs = RandomizedSearchCV(estimator,\n",
    "                        param_grid, \n",
    "                        n_jobs = -1,\n",
    "                        scoring = 'roc_auc',\n",
    "                        cv = 5,\n",
    "                        n_iter = 10,\n",
    "                        verbose = 4, \n",
    "                        random_state = 42)\n",
    "\n",
    "# fit it \n",
    "rs.fit(X_train_sp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### go for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_predictions = best_model.predict(X_train_sp)\n",
    "best_train_probas = best_model.predict_proba(X_train_sp)[:, 1]\n",
    "#and again \n",
    "best_test_predictions = best_model.predict(X_test_sp)\n",
    "best_test_probas = best_model.predict_proba(X_test_sp)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now after optimization\n",
    "b_n_nodes = []\n",
    "b_max_depths = []\n",
    "\n",
    "for tree in best_model.estimators_:\n",
    "    b_n_nodes.append(tree.tree_.node_count)\n",
    "    b_max_depths.append(tree.tree_.max_depth)\n",
    "\n",
    "print(f'Average number of nodes {int(np.mean(b_n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(b_max_depths))}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cm = confusion_matrix(y_test, best_test_predictions)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train ROC AUC Score: {roc_auc_score(y_train, best_train_probas)}')\n",
    "print(f'Test ROC AUC Score: {roc_auc_score(y_test, best_test_probas)}')\n",
    "print(f\"Baseline ROC AUC calling everyone successful: {roc_auc_score(y_test, [1 for _ in range(len(y_test))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_model = pd.DataFrame({'feature': features,\n",
    "                         'importance': best_model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "fi_model.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
