{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# KNN Classification Model for Kickstarter Success Prediction\n",
    "\n",
    "This notebook builds and evaluates K-Nearest Neighbors models to predict Kickstarter project success.\n",
    "\n",
    "## Notebook Structure\n",
    "1. **Setup & Data Loading** - Import libraries and load data\n",
    "2. **Feature Engineering** - Prepare features for modeling\n",
    "3. **Train/Test Split** - Create train and test sets\n",
    "4. **Baseline Model** - Simple rule-based prediction\n",
    "5. **Simple KNN Model** - KNN with default parameters\n",
    "6. **Tuned KNN Model** - KNN with hyperparameter optimization\n",
    "7. **Model Comparison** - Compare all models\n",
    "8. **Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, recall_score, f1_score, \n",
    "    roc_auc_score, precision_score, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Project path setup\n",
    "curr_dir = Path.cwd()\n",
    "project_root = curr_dir.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "from utils.feature_engineer_df import build_features, calculate_historical_success_rate\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build features using our utils\n",
    "build_features(\n",
    "    input_path=Path.cwd().resolve().parents[1] / \"data\" / \"cleaned\" / \"kickstarter_cleaned.csv\",\n",
    "    output_path=Path.cwd().resolve().parents[1] / \"data\" / \"feature\" / \"kickstarter_featured.csv\",\n",
    "    raw_path=Path.cwd().resolve().parents[1] / \"data\" / \"raw\" / \"ks-projects-201801.csv\",\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Load the featured dataset\n",
    "BASE_DIR = Path.cwd().resolve().parents[1]\n",
    "data_file = BASE_DIR / \"data\" / \"feature\" / \"kickstarter_featured.csv\"\n",
    "df = pd.read_csv(data_file, encoding='latin-1', low_memory=False)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Success Rate by Grouped Category\n",
    "category_success = df.groupby('main_category_grouped')['target'].apply(\n",
    "    lambda x: (x == 1).sum() / len(x) * 100\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x=category_success.values, y=category_success.index, palette='viridis')\n",
    "plt.title('Success Rate by Grouped Category (%)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Success Rate (%)')\n",
    "plt.ylabel('Main Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the feature (4 weeks)\n",
    "# df['hist_success_rate_4w'] = calculate_historical_success_rate(\n",
    "#     df, \n",
    "#     category_col='main_category_grouped',\n",
    "#     lookback_weeks=4\n",
    "# )\n",
    "\n",
    "# df['hist_success_rate_4w'] = df.groupby('main_category_grouped')['hist_success_rate_4w'].transform(\n",
    "#     lambda x: x.fillna(x.mean())\n",
    "# )\n",
    "\n",
    "# # Check the result\n",
    "# print(df[['main_category_grouped', 'launched', 'target', 'hist_success_rate_4w']].head(10))\n",
    "# print(f\"\\nNaN count: {df['hist_success_rate_4w'].isna().sum()}\")\n",
    "# print(f\"Mean rate: {df['hist_success_rate_4w'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 1. Distribution ---\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# df['hist_success_rate_4w'].dropna().hist(bins=50, ax=ax, edgecolor='black')\n",
    "# ax.axvline(df['hist_success_rate_4w'].mean(), color='red', linestyle='--', label=f\"Mean: {df['hist_success_rate_4w'].mean():.2%}\")\n",
    "# ax.set_xlabel('Historical Success Rate')\n",
    "# ax.set_ylabel('Count')\n",
    "# ax.set_title('Distribution of Historical Success Rates (4-week lookback)')\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # --- 2. By outcome (most important - shows predictive power) ---\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# df[df['target']==1]['hist_success_rate_4w'].dropna().hist(bins=30, alpha=0.6, label='Successful', color='green', ax=ax)\n",
    "# df[df['target']==0]['hist_success_rate_4w'].dropna().hist(bins=30, alpha=0.6, label='Failed', color='red', ax=ax)\n",
    "# ax.set_xlabel('Historical Success Rate')\n",
    "# ax.set_title('Does higher historical rate predict success?')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop (irrelevant or contain future information)\n",
    "columns_to_drop = [\n",
    "    'id',                      # Irrelevant identifier\n",
    "    'main_category',           # Will use grouped version\n",
    "    'deadline', 'launched',    # Will use derived features\n",
    "    'backers', 'usd_pledged_real', 'usd_pledged_bins',  # Future information (known after campaign)\n",
    "    'backers_per_pledged', 'backer_pledged_bins', 'pledged_per_category',  # Future information\n",
    "    'launched_year', 'deadline_year',  # Historical info, not seasonal\n",
    "]\n",
    "\n",
    "dfc = df.drop(columns=columns_to_drop)\n",
    "print(f\"Shape after dropping columns: {dfc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables (drop_first=True to avoid multicollinearity)\n",
    "df_sl = pd.get_dummies(df['launch_season'], prefix='sl_', drop_first=True, dtype=int)\n",
    "df_sd = pd.get_dummies(df['deadline_season'], prefix='sd_', drop_first=True, dtype=int)\n",
    "df_cat = pd.get_dummies(df['main_category_grouped'], prefix='cat_', drop_first=True, dtype=int)\n",
    "df_co = pd.get_dummies(df['continent'], prefix='co_', drop_first=True, dtype=int)\n",
    "\n",
    "# Combine all features\n",
    "dff = pd.concat([dfc, df_sl, df_sd, df_cat, df_co], axis=1)\n",
    "print(f\"Shape after encoding: {dff.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to soft-drop (redundant after encoding)\n",
    "columns_to_softdrop = [\n",
    "    'country',                  # Kept continent instead\n",
    "    'main_category_grouped',    # Already encoded\n",
    "    'continent',                # Already encoded\n",
    "    'launched_month',           # Using season instead\n",
    "    'deadline_month',           # Using season instead\n",
    "    'usd_goal_bins',            # Using goal_per_category\n",
    "    'category_goal_percentile', # Redundant with goal_per_category\n",
    "    'duration_bins',            # Using actual duration_days\n",
    "    'launch_season',            # Already encoded\n",
    "    'deadline_season',          # Already encoded\n",
    "]\n",
    "\n",
    "df_to_scale = dff.drop(columns=columns_to_softdrop, axis=1)\n",
    "print(f\"Final features ({len(df_to_scale.columns)} columns):\")\n",
    "print(df_to_scale.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix and target\n",
    "X = df_to_scale.drop(columns=['target'])\n",
    "y = df_to_scale['target']\n",
    "\n",
    "# Train-test split (stratified to maintain class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "print(f\"\\nClass balance in training set:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Baseline Model (Rule-Based)\n",
    "\n",
    "Before applying machine learning, let's establish a simple baseline using domain knowledge.\n",
    "\n",
    "**Baseline Rule**: Predict success (1) if the project belongs to Creative, Entertainment, or Tech categories (which may have higher success rates), otherwise predict failure (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "top_cols = ['cat__Creative', 'cat__Entertainment']\n",
    "\n",
    "def baseline_predict(row):\n",
    "    \"\"\"Simple rule-based prediction: success if in top categories\"\"\"\n",
    "    if row[top_cols].sum() > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply baseline model to test set\n",
    "y_pred_baseline = X_test.apply(baseline_predict, axis=1)\n",
    "\n",
    "# Calculate baseline metrics\n",
    "baseline_metrics = {\n",
    "    'Model': 'Baseline (Category Rule)',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1 Score': f1_score(y_test, y_pred_baseline),\n",
    "    'ROC AUC': np.nan  # Cannot calculate AUC without probabilities\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Rule: Predict success if project is in Creative, Entertainment, or Tech\")\n",
    "print()\n",
    "for metric, value in baseline_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric:12}: {value:.4f}\" if not np.isnan(value) else f\"{metric:12}: N/A\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "print(cm_baseline)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Failed', 'Success']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Simple KNN Model (Default Parameters)\n",
    "\n",
    "Now let's train a KNN model with default/reasonable parameters to see how much improvement ML provides over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column types for preprocessing\n",
    "num_cols = ['usd_goal_real', 'duration_days', 'goal_per_category']\n",
    "cat_cols = list(set(X.columns) - set(num_cols))\n",
    "\n",
    "print(f\"Numerical columns ({len(num_cols)}): {num_cols}\")\n",
    "print(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", \"passthrough\", cat_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple KNN with default parameters\n",
    "simple_knn = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"knn\", KNeighborsClassifier(\n",
    "        n_neighbors=9,      # Default\n",
    "        weights=\"uniform\",  # Default\n",
    "        metric=\"minkowski\", # Default\n",
    "        p=2                 # Euclidean distance\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Simple KNN...\")\n",
    "simple_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_simple = simple_knn.predict(X_test)\n",
    "y_proba_simple = simple_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "simple_knn_metrics = {\n",
    "    'Model': 'Simple KNN (k=9)',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_simple),\n",
    "    'Precision': precision_score(y_test, y_pred_simple),\n",
    "    'Recall': recall_score(y_test, y_pred_simple),\n",
    "    'F1 Score': f1_score(y_test, y_pred_simple),\n",
    "    'ROC AUC': roc_auc_score(y_test, y_proba_simple)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SIMPLE KNN MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Parameters: n_neighbors=5, weights='uniform', metric='minkowski', p=2\")\n",
    "print()\n",
    "for metric, value in simple_knn_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric:12}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm_simple = confusion_matrix(y_test, y_pred_simple)\n",
    "print(cm_simple)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_simple, target_names=['Failed', 'Success']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Tuned KNN Model (Hyperparameter Optimization)\n",
    "\n",
    "Now let's use RandomizedSearchCV to find optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for tuning\n",
    "tune_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    \"prep__num\": [MinMaxScaler(), RobustScaler()],\n",
    "    \"knn__n_neighbors\": [9, 11, 15, 17, 19, 21],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    \"knn__algorithm\": [\"auto\"],\n",
    "    \"knn__metric\": [\"minkowski\", \"chebyshev\"],\n",
    "    \"knn__p\": [1, 2, 3],  # 1=Manhattan, 2=Euclidean, 3=Minkowski\n",
    "}\n",
    "\n",
    "print(\"Parameter search space:\")\n",
    "for param, values in param_distributions.items():\n",
    "    print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "rs = RandomizedSearchCV(\n",
    "    tune_pipe,\n",
    "    param_distributions,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=4,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running RandomizedSearchCV (this may take a while)...\")\n",
    "rs.fit(X_train, y_train)\n",
    "print(\"\\nSearch complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters found\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST PARAMETERS FOUND\")\n",
    "print(\"=\" * 60)\n",
    "for param, value in rs.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV Score (ROC AUC): {rs.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on test set\n",
    "y_pred_tuned = rs.predict(X_test)\n",
    "y_proba_tuned = rs.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "tuned_knn_metrics = {\n",
    "    'Model': 'Tuned KNN (RandomizedSearchCV)',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_tuned),\n",
    "    'Precision': precision_score(y_test, y_pred_tuned),\n",
    "    'Recall': recall_score(y_test, y_pred_tuned),\n",
    "    'F1 Score': f1_score(y_test, y_pred_tuned),\n",
    "    'ROC AUC': roc_auc_score(y_test, y_proba_tuned)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TUNED KNN MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for metric, value in tuned_knn_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric:12}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "print(cm_tuned)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=['Failed', 'Success']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on best estimator for more robust evaluation\n",
    "scoring = ['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n",
    "cv_scores = cross_validate(rs.best_estimator_, X_test, y_test, scoring=scoring, cv=5)\n",
    "\n",
    "print(\"\\nCross-Validation Scores on Test Set:\")\n",
    "print(\"-\" * 40)\n",
    "for metric in scoring:\n",
    "    mean_score = cv_scores[f'test_{metric}'].mean()\n",
    "    std_score = cv_scores[f'test_{metric}'].std()\n",
    "    print(f\"{metric:12}: {mean_score:.4f} (+/- {std_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame([baseline_metrics, simple_knn_metrics, tuned_knn_metrics])\n",
    "results_df = results_df.set_index('Model')\n",
    "\n",
    "# Style the DataFrame\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df.style.format(\"{:.4f}\", na_rep=\"N/A\").highlight_max(axis=0, color='lightgreen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvements\n",
    "print(\"\\nIMPROVEMENT ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Simple KNN vs Baseline\n",
    "print(\"\\nSimple KNN vs Baseline:\")\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:\n",
    "    baseline_val = baseline_metrics[metric]\n",
    "    simple_val = simple_knn_metrics[metric]\n",
    "    improvement = (simple_val - baseline_val) / baseline_val * 100\n",
    "    print(f\"  {metric}: {improvement:+.2f}%\")\n",
    "\n",
    "# Tuned KNN vs Simple KNN\n",
    "print(\"\\nTuned KNN vs Simple KNN:\")\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']:\n",
    "    simple_val = simple_knn_metrics[metric]\n",
    "    tuned_val = tuned_knn_metrics[metric]\n",
    "    improvement = (tuned_val - simple_val) / simple_val * 100\n",
    "    print(f\"  {metric}: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 7.1 Visualization: Metrics Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing metrics across models\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "# Plot bars for each model\n",
    "bars1 = ax.bar(x - width, [baseline_metrics[m] for m in metrics_to_plot], \n",
    "               width, label='Baseline', color='#ff9999', edgecolor='black')\n",
    "bars2 = ax.bar(x, [simple_knn_metrics[m] for m in metrics_to_plot], \n",
    "               width, label='Simple KNN', color='#66b3ff', edgecolor='black')\n",
    "bars3 = ax.bar(x + width, [tuned_knn_metrics[m] for m in metrics_to_plot], \n",
    "               width, label='Tuned KNN', color='#99ff99', edgecolor='black')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Metric', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_to_plot)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 7.2 Visualization: ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Simple KNN ROC\n",
    "fpr_simple, tpr_simple, _ = roc_curve(y_test, y_proba_simple)\n",
    "auc_simple = roc_auc_score(y_test, y_proba_simple)\n",
    "ax.plot(fpr_simple, tpr_simple, 'b-', linewidth=2, \n",
    "        label=f'Simple KNN (AUC = {auc_simple:.4f})')\n",
    "\n",
    "# Tuned KNN ROC\n",
    "fpr_tuned, tpr_tuned, _ = roc_curve(y_test, y_proba_tuned)\n",
    "auc_tuned = roc_auc_score(y_test, y_proba_tuned)\n",
    "ax.plot(fpr_tuned, tpr_tuned, 'g-', linewidth=2, \n",
    "        label=f'Tuned KNN (AUC = {auc_tuned:.4f})')\n",
    "\n",
    "# Random classifier baseline\n",
    "ax.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 7.3 Visualization: Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Simple KNN PR\n",
    "precision_simple, recall_simple, _ = precision_recall_curve(y_test, y_proba_simple)\n",
    "pr_auc_simple = auc(recall_simple, precision_simple)\n",
    "ax.plot(recall_simple, precision_simple, 'b-', linewidth=2, \n",
    "        label=f'Simple KNN (PR AUC = {pr_auc_simple:.4f})')\n",
    "\n",
    "# Tuned KNN PR\n",
    "precision_tuned, recall_tuned, _ = precision_recall_curve(y_test, y_proba_tuned)\n",
    "pr_auc_tuned = auc(recall_tuned, precision_tuned)\n",
    "ax.plot(recall_tuned, precision_tuned, 'g-', linewidth=2, \n",
    "        label=f'Tuned KNN (PR AUC = {pr_auc_tuned:.4f})')\n",
    "\n",
    "# Baseline (proportion of positives)\n",
    "baseline_pr = y_test.mean()\n",
    "ax.axhline(y=baseline_pr, color='r', linestyle='--', linewidth=1, \n",
    "           label=f'Baseline (Positive Rate = {baseline_pr:.4f})')\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curves Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 7.4 Visualization: Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "cms = [cm_baseline, cm_simple, cm_tuned]\n",
    "titles = ['Baseline Model', 'Simple KNN', 'Tuned KNN']\n",
    "\n",
    "for ax, cm, title in zip(axes, cms, titles):\n",
    "    # Normalize confusion matrix for percentages\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Failed', 'Success'],\n",
    "                yticklabels=['Failed', 'Success'])\n",
    "    \n",
    "    # Add percentages as secondary annotation\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j + 0.5, i + 0.75, f'({cm_normalized[i, j]:.1%})',\n",
    "                    ha='center', va='center', fontsize=9, color='gray')\n",
    "    \n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "    ax.set_ylabel('Actual', fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Confusion Matrices Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 7.5 Visualization: Hyperparameter Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze RandomizedSearchCV results\n",
    "cv_results = pd.DataFrame(rs.cv_results_)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Score by number of neighbors\n",
    "ax1 = axes[0, 0]\n",
    "neighbor_scores = cv_results.groupby('param_knn__n_neighbors')['mean_test_score'].agg(['mean', 'std']).reset_index()\n",
    "ax1.errorbar(neighbor_scores['param_knn__n_neighbors'], neighbor_scores['mean'], \n",
    "             yerr=neighbor_scores['std'], marker='o', capsize=5, linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Neighbors (k)', fontsize=11)\n",
    "ax1.set_ylabel('Mean CV Score (ROC AUC)', fontsize=11)\n",
    "ax1.set_title('Performance by Number of Neighbors', fontsize=12, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Score by weight type\n",
    "ax2 = axes[0, 1]\n",
    "sns.boxplot(x='param_knn__weights', y='mean_test_score', data=cv_results, ax=ax2, palette='Set2')\n",
    "ax2.set_xlabel('Weight Type', fontsize=11)\n",
    "ax2.set_ylabel('Mean CV Score (ROC AUC)', fontsize=11)\n",
    "ax2.set_title('Performance by Weight Type', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Score by scaler\n",
    "ax3 = axes[1, 0]\n",
    "cv_results['scaler_name'] = cv_results['param_prep__num'].astype(str).str.extract(r'([A-Za-z]+)Scaler')\n",
    "sns.boxplot(x='scaler_name', y='mean_test_score', data=cv_results, ax=ax3, palette='Set3')\n",
    "ax3.set_xlabel('Scaler Type', fontsize=11)\n",
    "ax3.set_ylabel('Mean CV Score (ROC AUC)', fontsize=11)\n",
    "ax3.set_title('Performance by Scaler', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Score by p value (Minkowski distance parameter)\n",
    "ax4 = axes[1, 1]\n",
    "p_scores = cv_results.groupby('param_knn__p')['mean_test_score'].agg(['mean', 'std']).reset_index()\n",
    "ax4.bar(p_scores['param_knn__p'].astype(str), p_scores['mean'], \n",
    "        yerr=p_scores['std'], capsize=5, color=['#ff9999', '#66b3ff', '#99ff99'], edgecolor='black')\n",
    "ax4.set_xlabel('Minkowski p Parameter', fontsize=11)\n",
    "ax4.set_ylabel('Mean CV Score (ROC AUC)', fontsize=11)\n",
    "ax4.set_title('Performance by Distance Metric (p)', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticklabels(['p=1 (Manhattan)', 'p=2 (Euclidean)', 'p=3'])\n",
    "\n",
    "plt.suptitle('Hyperparameter Search Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Baseline (Rule-Based):  F1={baseline_metrics['F1 Score']:.4f}\")\n",
    "print(f\"   Simple KNN (k=5):       F1={simple_knn_metrics['F1 Score']:.4f}, ROC AUC={simple_knn_metrics['ROC AUC']:.4f}\")\n",
    "print(f\"   Tuned KNN:              F1={tuned_knn_metrics['F1 Score']:.4f}, ROC AUC={tuned_knn_metrics['ROC AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n2. KEY FINDINGS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Compare models\n",
    "best_model = max([('Baseline', baseline_metrics['F1 Score']),\n",
    "                  ('Simple KNN', simple_knn_metrics['F1 Score']),\n",
    "                  ('Tuned KNN', tuned_knn_metrics['F1 Score'])], key=lambda x: x[1])\n",
    "print(f\"   - Best performing model: {best_model[0]}\")\n",
    "\n",
    "# Improvement from baseline\n",
    "baseline_f1 = baseline_metrics['F1 Score']\n",
    "tuned_f1 = tuned_knn_metrics['F1 Score']\n",
    "improvement = (tuned_f1 - baseline_f1) / baseline_f1 * 100\n",
    "print(f\"   - Tuned KNN improvement over baseline: {improvement:+.2f}% (F1 Score)\")\n",
    "\n",
    "# Tuning benefit\n",
    "simple_auc = simple_knn_metrics['ROC AUC']\n",
    "tuned_auc = tuned_knn_metrics['ROC AUC']\n",
    "tuning_improvement = (tuned_auc - simple_auc) / simple_auc * 100\n",
    "print(f\"   - Hyperparameter tuning benefit: {tuning_improvement:+.2f}% (ROC AUC)\")\n",
    "\n",
    "print(\"\\n3. BEST HYPERPARAMETERS FOUND:\")\n",
    "print(\"-\" * 40)\n",
    "for param, value in rs.best_params_.items():\n",
    "    clean_param = param.replace('knn__', '').replace('prep__', '')\n",
    "    print(f\"   - {clean_param}: {value}\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "if tuned_auc < 0.65:\n",
    "    print(\"   - KNN shows limited predictive power for this problem\")\n",
    "    print(\"   - Consider trying other algorithms (Random Forest, Gradient Boosting, etc.)\")\n",
    "    print(\"   - Feature engineering may need improvement\")\n",
    "else:\n",
    "    print(\"   - KNN provides reasonable predictive performance\")\n",
    "    print(\"   - Consider ensemble methods to potentially improve further\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "---\n",
    "### Final Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final comparison table\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "display(results_df.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
