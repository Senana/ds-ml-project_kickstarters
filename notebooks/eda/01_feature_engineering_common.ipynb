{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Feature Engineering Exploration\n",
    "We use this notebook to consolidate the initial transformation of our initial columns into more informative features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "From the data-cleaning, we can now import our cleaned files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Files as DataFrames\n",
    "BASE_DIR = Path.cwd().resolve().parents[1]\n",
    "data_file_1 = BASE_DIR / \"data\" / \"cleaned\" / \"kickstarter_cleaned_with_cancelled.csv\"\n",
    "data_file_2 = BASE_DIR / \"data\" / \"cleaned\" / \"kickstarter_cleaned.csv\"\n",
    "\n",
    "filepath_1 = Path(data_file_1)\n",
    "filepath_2 = Path(data_file_2)\n",
    "\n",
    "df1 = pd.read_csv(filepath_1, encoding='latin-1', low_memory=False)\n",
    "df2 = pd.read_csv(filepath_2, low_memory=False)\n",
    "\n",
    "logger.info(f\"Loaded {len(df1)} rows and {len(df1.columns)} columns\")\n",
    "logger.info(f\"Loaded {len(df2)} rows and {len(df2.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### We are looking into potential outlier cleaning for the numerical money-columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for projects which are not \"0\" in either - as these count as \"not happened\"\n",
    "df2_clean = df2[(df2['usd_pledged_real'] > 0) & (df2['usd_goal_real'] > 0)].copy()\n",
    "#checking the distributions\n",
    "df2_clean['usd_goal_real'].describe().round(2)\n",
    "df2_clean['usd_goal_real'].min()\n",
    "#adding the info to the logger \n",
    "logger.info(f\"Loaded {len(df1)} rows and {len(df1.columns)} columns\")\n",
    "logger.info(f\"Loaded {len(df2)} rows and {len(df2.columns)} columns\")\n",
    "# checking out the \"goal\" again\n",
    "df2_clean['usd_goal_real'].describe().round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We decide to bin the data by quantiles. The curves are power-law distributions, so we decided to prioritise having comparable amounts of projects in each bin over having a uniform step size between bins. \n",
    "\n",
    "Using quartiles ensures that. \n",
    "\n",
    "We also decided on a small number of bins to make it easier for the feature engineering later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the binning for both goals and pledged \n",
    "df2_clean.loc[:, 'usd_goal_bins'] = pd.qcut(df2_clean['usd_goal_real'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "df2_clean.loc[:, 'usd_pledged_bins'] = pd.qcut(df2_clean['usd_pledged_real'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the columns after doing this \n",
    "df2_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data \n",
    "df2_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Creating further bins "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##### Categories of Categories\n",
    "We want to aggregate categories in fewer bins. We checked the curves and manually ascribe categories by \"is this the same type of thing\" to end up with a consolidated set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping countries by main_category\n",
    "category_map = {\n",
    "    'Art': 'Creative',\n",
    "    'Comics': 'Creative',\n",
    "    'Crafts': 'Creative',\n",
    "    'Dance': 'Creative',\n",
    "    'Design': 'Creative',\n",
    "    'Fashion': 'Consumer',\n",
    "    'Film & Video': 'Entertainment',\n",
    "    'Games': 'Entertainment',\n",
    "    'Music': 'Entertainment',\n",
    "    'Photography': 'Creative',\n",
    "    'Publishing': 'Creative',\n",
    "    'Technology': 'Tech',\n",
    "    'Food': 'Consumer',\n",
    "    'Journalism': 'Other',\n",
    "    'Theater': 'Entertainment'\n",
    "}\n",
    "\n",
    "df2_clean.loc[:, 'main_category_grouped'] = df2_clean['main_category'].map(category_map).fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-doing the datetime transformation as it was lost by exporting to csv\n",
    "df2_clean.loc[:, \"launched\"] = pd.to_datetime(df2_clean[\"launched\"], errors=\"coerce\")\n",
    "df2_clean.loc[:, \"deadline\"] = pd.to_datetime(df2_clean[\"deadline\"], errors=\"coerce\")\n",
    "\n",
    "type(df2_clean['deadline'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### Countries by continent\n",
    "\n",
    "We agree to aggregate countries by continent. \n",
    "\n",
    "This is not the only way to do this, as within a continent countries can have vastly different properties. \n",
    "\n",
    "However, we decided to go for this granularity as the similarities by continent seem sufficient.\n",
    "\n",
    "If we did other bins - like \"top money countries\" or \"top number of projects countries\", these could easily change later with new data coming in, so they don't seem ideal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.loc[:, 'country'] = df2_clean['country'].replace('N,0\"', 'NO')\n",
    "\n",
    "#grouping countries by continents\n",
    "continent_map = {\n",
    "    'US': 'North America', 'CA': 'North America', 'MX': 'North America',\n",
    "    'GB': 'Europe', 'DE': 'Europe', 'FR': 'Europe', 'IT': 'Europe',\n",
    "    'ES': 'Europe', 'NL': 'Europe', 'IE': 'Europe', 'SE': 'Europe',\n",
    "    'CH': 'Europe', 'AT': 'Europe', 'DK': 'Europe', 'BE': 'Europe', 'LU': 'Europe', 'NO': 'Europe',\n",
    "    'AU': 'Oceania', 'NZ': 'Oceania', \n",
    "    'JP': 'Asia', 'SG': 'Asia', 'HK': 'Asia',\n",
    "}\n",
    "\n",
    "df2_clean.loc[:, 'continent'] = df2_clean['country'].map(continent_map).fillna('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### Time categories \n",
    "We go for further categories within time, as we have the hypothesis that seasonality could be relevant for success.\n",
    "\n",
    "Thus, we add both month and year to our category set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year and month as separate columns (still datetime)\n",
    "df2_clean.loc[:, 'deadline_year'] = df2_clean['deadline'].apply(lambda x: x.year)      #dividing into months and years\n",
    "df2_clean.loc[:, 'deadline_month'] = df2_clean['deadline'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for launched: Create month and year as separate columns \n",
    "df2_clean.loc[:, 'launched_year'] = df2_clean['launched'].apply(lambda x: x.year)          #dividing into months and years\n",
    "df2_clean.loc[:, 'launched_month'] = df2_clean['launched'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Continuing with time, let's make sense of \"Duration\"\n",
    "\n",
    "Issue: This data is highly irregular. It's neither skewed nor normally distributed, it has enormous spikes at \"one month\" and also a smaller at the \"two month\" mark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out if there's numerical bins adequate \n",
    "df2_clean['duration_days'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "As there are no sensible bins coming from this, we will cut arbitrary bins to have workable categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write bins as \"two week slots\", avoiding the spike at 29 days \n",
    "bins = [15, 29, 45, 60, 75]\n",
    "#label it \n",
    "labels = ['2 weeks', '4 weeks', '6 weeks', '8 weeks']\n",
    "#add that to the dataframe\n",
    "df2_clean.loc[:, 'duration_bins'] = pd.cut(df2_clean['duration_days'], bins=bins, labels=labels)\n",
    "#check it out how it looks like \n",
    "df2_clean['duration_bins'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-check the columns again \n",
    "df2_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "##### Backers\n",
    "We might not be able to use it (as it predicts the future) but we still played around with some potential measures regarding backers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backers/pledged\n",
    "df2_clean.loc[:, 'backers_per_pledged'] = df2_clean['backers'] / df2_clean['usd_pledged_real']\n",
    "df2_clean.loc[:, 'backer_pledged_bins'] = pd.qcut(df2_clean['backers_per_pledged'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the df again\n",
    "df2_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pledged bin per category\n",
    "df2_clean.loc[:, 'pledged_per_category'] = df2_clean.groupby('main_category')['usd_pledged_real'].transform('mean')\n",
    "df2_clean.loc[:, 'goal_per_category'] = df2_clean.groupby('main_category')['usd_goal_real'].transform('mean')\n",
    "\n",
    "# category related bins\n",
    "df2_clean.loc[:, 'category_goal_percentile'] = df2_clean.groupby('main_category_grouped')['usd_goal_real'].transform(lambda x: pd.qcut(x, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']))\n",
    "df2_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between category goal percentail and goal bins\n",
    "df2_test = df2_clean.query('category_goal_percentile != usd_goal_bins')\n",
    "df2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_season(month: Optional[int]) -> Optional[str]:\n",
    "    \"\"\"Convert month to season.\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "df2_clean.loc[:, 'launch_season'] = df2_clean['launched_month'].apply(convert_season)\n",
    "df2_clean.loc[:, 'deadline_season'] = df2_clean['deadline_month'].apply(convert_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Finalising "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Save the created dataset as new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = Path.cwd().resolve().parents[1]\n",
    "RAW_PATH = BASE_DIR / \"data\" / \"feature\"\n",
    "FEATURED_PATH = BASE_DIR / \"data\" / \"feature\"\n",
    "\n",
    "RAW_DATA_PATH = Path(RAW_PATH)\n",
    "FEATURED_DATA_PATH = Path(FEATURED_PATH)\n",
    "\n",
    "# Create output directory if not exists\n",
    "FEATURED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save main dataset\n",
    "main_path = FEATURED_DATA_PATH / 'kickstarter_featured.csv'\n",
    "df2_clean.to_csv(main_path, index=False)\n",
    "print(f\" Saved: {main_path}\")\n",
    "\n",
    "# # Save dataset with cancelled\n",
    "# cancelled_path = FEATURED_DATA_PATH / 'kickstarter_featured_with_cancelled.csv'\n",
    "# df_with_cancelled.to_csv(cancelled_path, index=False)\n",
    "# print(f\"\\n Saved: {cancelled_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
